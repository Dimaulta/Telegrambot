import Vapor
import Foundation

final class SoranowBotController {
    func handleWebhook(_ req: Request) async throws -> Response {
        req.logger.info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        req.logger.info("üîî SoranowBot webhook hit!")
        req.logger.info("Method: \(req.method), Path: \(req.url.path)")
        
        let token = Environment.get("SORANOWBOT_TOKEN")
        guard let token = token, token.isEmpty == false else {
            req.logger.error("SORANOWBOT_TOKEN is missing")
            return Response(status: .internalServerError)
        }

        let rawBody = req.body.string ?? ""
        req.logger.info("üì¶ Raw body length: \(rawBody.count) characters")
        if rawBody.count > 0 && rawBody.count < 500 {
            req.logger.debug("Raw body: \(rawBody)")
        }

        req.logger.info("üîç Decoding SoranowBotUpdate...")
        let update = try? req.content.decode(SoranowBotUpdate.self)
        if update == nil { 
            req.logger.error("‚ùå Failed to decode SoranowBotUpdate - check raw body above")
        return Response(status: .ok)
    }
        req.logger.info("‚úÖ SoranowBotUpdate decoded successfully")

        guard let message = update?.message else {
            req.logger.warning("‚ö†Ô∏è No message in update (update_id: \(update?.update_id ?? -1))")
            return Response(status: .ok)
        }
        let text = message.text ?? ""
        req.logger.info("üì® Incoming message - chatId=\(message.chat.id), text length=\(text.count)")
        if !text.isEmpty {
            req.logger.info("üìù Message text: \(text.prefix(200))")
        }

        req.logger.info("üîç Checking for Sora URL in message...")
        guard let shareUrl = extractSoraShareURL(from: text) else {
            req.logger.info("‚ÑπÔ∏è No Sora URL found in message (text: \(text.prefix(100)))")
            return Response(status: .ok)
        }
        req.logger.info("Detected Sora share URL: \(shareUrl)")

        // –í–ê–ñ–ù–û: Telegram webhook –¥–æ–ª–∂–µ–Ω –±—ã—Å—Ç—Ä–æ –æ—Ç–≤–µ—Ç–∏—Ç—å (–≤ —Ç–µ—á–µ–Ω–∏–µ 60 —Å–µ–∫—É–Ω–¥)
        // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É, –æ–±—Ä–∞–±–æ—Ç–∫—É –¥–µ–ª–∞–µ–º –≤ —Ñ–æ–Ω–µ
        // –°–æ—Ö—Ä–∞–Ω—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –∑–∞–¥–∞—á–∏
        let chatId = message.chat.id
        let client = req.client
        let logger = req.logger
        
        Task { [token, shareUrl, chatId] in
            logger.info("üöÄ Background task started for URL: \(shareUrl)")
            do {
                // –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –æ –Ω–∞—á–∞–ª–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
                logger.info("üì§ Sending 'processing' message to user...")
                _ = try? await sendTelegramMessage(token: token, chatId: chatId, text: "‚è≥ –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Å—Å—ã–ª–∫—É, –ø–æ–¥–æ–∂–¥–∏ –Ω–µ–º–Ω–æ–≥–æ...", client: client)
                logger.info("‚úÖ 'Processing' message sent")
                
                // –°–æ–∑–¥–∞—ë–º –Ω–æ–≤—ã–π Request –¥–ª—è —Ñ–æ–Ω–æ–≤–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º eventLoop –∏–∑ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ req)
                logger.info("üîß Creating background request...")
                let backgroundReq = Request(application: req.application, method: .GET, url: URI(string: "/"), on: req.eventLoop)
                logger.info("‚úÖ Background request created, calling fetchDirectSoraVideoUrl...")
                
                let directUrl = try await fetchDirectSoraVideoUrl(from: shareUrl, req: backgroundReq)
                logger.info("‚úÖ fetchDirectSoraVideoUrl completed, extracted URL length=\(directUrl.count), URL: \(directUrl.prefix(200))...")
                
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Å—Å—ã–ª–∫–∞ –Ω–∞ –≤–∏–¥–µ–æ, –∞ –Ω–µ –∏—Å—Ö–æ–¥–Ω–∞—è —Å—Å—ã–ª–∫–∞ –Ω–∞ Sora
                guard directUrl.contains("videos.openai.com") else {
                    logger.error("Extracted URL is not a video URL: \(directUrl)")
                    _ = try? await sendTelegramMessage(token: token, chatId: chatId, text: "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑, –º–æ–π —Ö–æ—Ä–æ—à–∏–π üíï", client: client)
                    return
                }
                
                // –í–ê–ñ–ù–û: /az/files/{uuid}/raw —Å—Å—ã–ª–∫–∏ - —ç—Ç–æ —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –æ—Ç nosorawm.app!
                // –û–Ω–∏ –º–æ–≥—É—Ç –≤–æ–∑–≤—Ä–∞—â–∞—Ç—å 403 –ø—Ä–∏ –ø—Ä—è–º–æ–π –ø—Ä–æ–≤–µ—Ä–∫–µ (SAS —Ç–æ–∫–µ–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –ø—É—Ç–∏),
                // –Ω–æ –¥–ª—è Telegram API –º–æ–≥—É—Ç —Ä–∞–±–æ—Ç–∞—Ç—å. –ü–æ—ç—Ç–æ–º—É –ù–ï –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏—Ö, –∞ —Å—Ä–∞–∑—É –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º!
                if directUrl.contains("/az/files/") && directUrl.contains("/raw") && !directUrl.contains("/drvs/") {
                    logger.info("‚úÖ Found /az/files/{uuid}/raw URL (like nosorawm.app format) - sending directly without test (may work for Telegram API even if direct test fails)")
                    // –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –∏ fallback - –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º –Ω–∞–ø—Ä—è–º—É—é!
                }
                
                logger.info("üì§ Sending final URL to Telegram: \(directUrl.prefix(200))...")
                let sent = try await sendTelegramMessage(token: token, chatId: chatId, text: directUrl, client: client)
                if sent {
                    logger.info("‚úÖ Successfully sent URL to Telegram")
                } else {
                    logger.error("‚ùå Failed to send URL to Telegram")
                }
            } catch {
                logger.error("Failed to extract/send direct URL: \(String(describing: error))")
                let errorMsg = error.localizedDescription
                let userMsg: String
                if errorMsg.contains("Cloudflare") {
                    userMsg = "Cloudflare –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π:\n1) –í–∫–ª—é—á–∏—Ç—å VPN –Ω–∞ –°–®–ê\n2) –û–±–Ω–æ–≤–∏—Ç—å –∫—É–∫–∏ –≤ config/.env\n3) –ü–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –ø–æ–∑–∂–µ"
                } else {
                    userMsg = "–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É. –ü–æ–ø—Ä–æ–±—É–π –µ—â—ë —Ä–∞–∑, –º–æ–π —Ö–æ—Ä–æ—à–∏–π üíï"
                }
                _ = try? await sendTelegramMessage(token: token, chatId: chatId, text: userMsg, client: client)
            }
        }

        // –í–æ–∑–≤—Ä–∞—â–∞–µ–º –æ—Ç–≤–µ—Ç —Å—Ä–∞–∑—É, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä–æ–¥–æ–ª–∂–∞–µ—Ç—Å—è –≤ —Ñ–æ–Ω–µ
        req.logger.info("‚úÖ Webhook processed, returning OK response (background task started)")
        req.logger.info("‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê")
        return Response(status: .ok)
    }
}

private func extractSoraShareURL(from text: String) -> String? {
    if let detector = try? NSDataDetector(types: NSTextCheckingResult.CheckingType.link.rawValue) {
        let range = NSRange(text.startIndex..<text.endIndex, in: text)
        for match in detector.matches(in: text, options: [], range: range) {
            if let r = Range(match.range, in: text) {
                let urlString = String(text[r])
                if urlString.contains("sora.chatgpt.com/p/") {
                    return urlString
                }
            }
        }
    }
    if let r = text.range(of: "https://sora.chatgpt.com/p/") {
        return String(text[r.lowerBound...].split(separator: " ").first ?? Substring(""))
    }
    return nil
}

/// –ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScrapingBee API (–Ω–∞–¥—ë–∂–Ω–æ –æ–±—Ö–æ–¥–∏—Ç Cloudflare)
private func fetchViaScrapingBee(url: String, apiKey: String, req: Request) async throws -> String {
    // render_js=true –Ω—É–∂–µ–Ω –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–ª–Ω–æ–≥–æ HTML —Å __NEXT_DATA__ (Next.js —Ä–µ–Ω–¥–µ—Ä–∏—Ç –µ–≥–æ —á–µ—Ä–µ–∑ JS)
    // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º wait –¥–æ 60000ms (60 —Å–µ–∫—É–Ω–¥) –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS –∏ –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__
    // –î–æ–±–∞–≤–ª—è–µ–º wait_for –¥–ª—è –æ–∂–∏–¥–∞–Ω–∏—è –ø–æ—è–≤–ª–µ–Ω–∏—è __NEXT_DATA__ —Å–∫—Ä–∏–ø—Ç–∞
    let encodedUrl = url.addingPercentEncoding(withAllowedCharacters: .urlQueryAllowed) ?? url
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è: –ø—Ä–æ–±—É–µ–º —Å wait_for —Å–µ–ª–µ–∫—Ç–æ—Ä–æ–º –¥–ª—è __NEXT_DATA__
    // –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç, ScrapingBee –±—É–¥–µ—Ç –∂–¥–∞—Ç—å –∑–∞–¥–∞–Ω–Ω–æ–µ –≤—Ä–µ–º—è
    var apiUrl = "https://app.scrapingbee.com/api/v1/?api_key=\(apiKey)&url=\(encodedUrl)&render_js=true&wait=60000&premium_proxy=true&block_ads=true"
    
    // –ü—Ä–æ–±—É–µ–º –¥–æ–±–∞–≤–∏—Ç—å wait_for (–µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)
    // –ñ–¥—ë–º –ø–æ—è–≤–ª–µ–Ω–∏—è —Å–∫—Ä–∏–ø—Ç–∞ —Å id __NEXT_DATA__
    let waitForSelector = "script#__NEXT_DATA__"
    if let encodedSelector = waitForSelector.addingPercentEncoding(withAllowedCharacters: .urlQueryAllowed) {
        apiUrl += "&wait_for=\(encodedSelector)"
        req.logger.debug("Using wait_for selector: \(waitForSelector)")
    }
    
    req.logger.debug("ScrapingBee API URL: \(apiUrl)")
    
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º HTTP –∫–ª–∏–µ–Ω—Ç Vapor –¥–ª—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ —Å —Ç–∞–π–º–∞—É—Ç–æ–º
    let client = req.client
    let uri = URI(string: apiUrl)
    
    do {
        let response = try await client.get(uri).get()
        
        guard response.status == HTTPStatus.ok else {
            let bodyStr = response.body?.getString(at: 0, length: response.body?.readableBytes ?? 0, encoding: .utf8) ?? ""
            req.logger.error("ScrapingBee API returned status \(response.status.code): \(bodyStr.prefix(200))")
            throw Abort(.badRequest, reason: "ScrapingBee API returned status \(response.status.code)")
        }
        
        guard let body = response.body else {
            throw Abort(.badRequest, reason: "ScrapingBee returned empty body")
        }
        
        guard let html = body.getString(at: 0, length: body.readableBytes, encoding: .utf8), !html.isEmpty else {
            throw Abort(.badRequest, reason: "ScrapingBee returned empty HTML")
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –≤–µ—Ä–Ω—É–ª –ª–∏ ScrapingBee JSON —Å –æ—à–∏–±–∫–æ–π –≤–º–µ—Å—Ç–æ HTML
        if html.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines).hasPrefix("{") {
            req.logger.warning("ScrapingBee returned JSON instead of HTML (possible error): \(html.prefix(200))")
            throw Abort(.badRequest, reason: "ScrapingBee returned error response")
        }
        
        req.logger.info("ScrapingBee returned HTML (length: \(html.count))")
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ __NEXT_DATA__ –≤ HTML
        let hasNextData = html.contains("__NEXT_DATA__")
        req.logger.info("ScrapingBee HTML contains __NEXT_DATA__: \(hasNextData)")
        if !hasNextData {
            req.logger.warning("‚ö†Ô∏è __NEXT_DATA__ not found in ScrapingBee HTML - JS rendering may be incomplete")
        }
        
        return html
    } catch {
        req.logger.error("ScrapingBee request failed: \(error)")
        throw Abort(.badRequest, reason: "ScrapingBee request failed: \(error.localizedDescription)")
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ Playwright-—Å–µ—Ä–≤–∏—Å (–ª–æ–∫–∞–ª—å–Ω—ã–π Docker-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä)
private func fetchViaPlaywright(url: String, serviceUrl: String, req: Request) async throws -> String {
    // Playwright-—Å–µ—Ä–≤–∏—Å —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ localhost:3000 (–∏–ª–∏ –∑–∞–¥–∞–Ω —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è)
    let apiUrl = "\(serviceUrl)/fetch"
    
    req.logger.info("üé≠ Calling Playwright service at \(apiUrl)...")
    
    let client = req.client
    
    var headers = HTTPHeaders()
    headers.contentType = .json
    
    let uri = URI(string: apiUrl)
    
    do {
        req.logger.info("üì§ Sending POST request to Playwright service with URL: \(url.prefix(100))...")
        req.logger.info("‚è±Ô∏è This may take up to 60 seconds (waiting for page load and __NEXT_DATA__)...")
        
        let response = try await client.post(uri, headers: headers) { req in
            try req.content.encode(["url": url] as [String: String])
        }.get()
        
        guard response.status == HTTPStatus.ok else {
            let bodyStr = response.body?.getString(at: 0, length: response.body?.readableBytes ?? 0, encoding: .utf8) ?? ""
            req.logger.error("Playwright service returned status \(response.status.code): \(bodyStr.prefix(200))")
            throw Abort(.badRequest, reason: "Playwright service returned status \(response.status.code)")
        }
        
        guard let body = response.body else {
            throw Abort(.badRequest, reason: "Playwright service returned empty body")
        }
        
        guard let bodyString = body.getString(at: 0, length: body.readableBytes, encoding: .utf8), !bodyString.isEmpty else {
            throw Abort(.badRequest, reason: "Playwright service returned empty response")
        }
        
        // –ü–∞—Ä—Å–∏–º JSON –æ—Ç–≤–µ—Ç
        struct PlaywrightResponse: Codable {
            let success: Bool
            let html: String?
            let hasNextData: Bool?
            let length: Int?
            let error: String?
        }
        
        guard let data = bodyString.data(using: .utf8),
              let responseObj = try? JSONDecoder().decode(PlaywrightResponse.self, from: data) else {
            throw Abort(.badRequest, reason: "Failed to parse Playwright service response")
        }
        
        guard responseObj.success, let html = responseObj.html, !html.isEmpty else {
            let errorMsg = responseObj.error ?? "Unknown error"
            req.logger.error("Playwright service returned error: \(errorMsg)")
            throw Abort(.badRequest, reason: "Playwright service error: \(errorMsg)")
        }
        
        req.logger.info("‚úÖ Playwright service returned HTML (length: \(html.count))")
        if let hasNextData = responseObj.hasNextData {
            req.logger.info("‚úÖ Playwright service HTML contains __NEXT_DATA__: \(hasNextData)")
            if hasNextData {
                req.logger.info("üéâ Playwright service successfully found __NEXT_DATA__! This should give us the correct UUID!")
            }
        }
        
        return html
    } catch {
        req.logger.error("Playwright service request failed: \(error)")
        throw Abort(.badRequest, reason: "Playwright service request failed: \(error.localizedDescription)")
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ ScraperAPI (–ë–ï–°–ü–õ–ê–¢–ù–û: 5000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü!)
private func fetchViaScraperAPI(url: String, apiKey: String, req: Request) async throws -> String {
    // ScraperAPI –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∏ –æ–±—Ö–æ–¥ Cloudflare
    // –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω: 5000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü
    let encodedUrl = url.addingPercentEncoding(withAllowedCharacters: .urlQueryAllowed) ?? url
    // render=true –≤–∫–ª—é—á–∞–µ—Ç JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ (–≤–∞–∂–Ω–æ –¥–ª—è __NEXT_DATA__)
    // premium=true –º–æ–∂–µ—Ç —Ç—Ä–µ–±–æ–≤–∞—Ç—å –ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω, –ø–æ—ç—Ç–æ–º—É –ø—Ä–æ–±—É–µ–º –±–µ–∑ –Ω–µ–≥–æ —Å–Ω–∞—á–∞–ª–∞
    // device_type=desktop –¥–ª—è –ª—É—á—à–µ–π –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
    // wait_time —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –æ–∂–∏–¥–∞–Ω–∏—è JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ (–≤ —Å–µ–∫—É–Ω–¥–∞—Ö, –º–∞–∫—Å–∏–º—É–º –æ–±—ã—á–Ω–æ 60 –¥–ª—è –±–µ—Å–ø–ª–∞—Ç–Ω–æ–≥–æ –ø–ª–∞–Ω–∞)
    // –≠—Ç–æ –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–≥—Ä—É–∂–∞–µ—Ç—Å—è –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ
    // –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 35 —Å–µ–∫—É–Ω–¥ –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ __NEXT_DATA__
    let apiUrl = "http://api.scraperapi.com?api_key=\(apiKey)&url=\(encodedUrl)&render=true&device_type=desktop&wait_time=35"
    
    req.logger.debug("ScraperAPI URL: \(apiUrl.prefix(200))...")
    
    let client = req.client
    let uri = URI(string: apiUrl)
    
    do {
        let response = try await client.get(uri).get()
        
        guard response.status == HTTPStatus.ok else {
            let bodyStr = response.body?.getString(at: 0, length: response.body?.readableBytes ?? 0, encoding: .utf8) ?? ""
            req.logger.error("ScraperAPI returned status \(response.status.code): \(bodyStr.prefix(200))")
            throw Abort(.badRequest, reason: "ScraperAPI returned status \(response.status.code)")
        }
        
        guard let body = response.body else {
            throw Abort(.badRequest, reason: "ScraperAPI returned empty body")
        }
        
        guard let html = body.getString(at: 0, length: body.readableBytes, encoding: .utf8), !html.isEmpty else {
            throw Abort(.badRequest, reason: "ScraperAPI returned empty HTML")
        }
        
        req.logger.info("ScraperAPI returned HTML (length: \(html.count))")
        
        // –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ __NEXT_DATA__ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        let hasNextData = html.contains("__NEXT_DATA__") || 
                         html.contains("__next_data__") || 
                         html.contains("__NEXT_DATA") ||
                         html.contains("NEXT_DATA") ||
                         html.contains("%5B%5B__NEXT_DATA__%5D%5D") ||
                         html.contains("&#x5f;&#x5f;NEXT_DATA&#x5f;&#x5f;")
        
        req.logger.info("ScraperAPI HTML contains __NEXT_DATA__ (any format): \(hasNextData)")
        
        // –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é extractNextDataJSON
        if !hasNextData {
            req.logger.warning("‚ö†Ô∏è __NEXT_DATA__ not found with simple contains check, trying deep extraction...")
            if let nextData = extractNextDataJSON(from: html) {
                req.logger.info("‚úÖ Found __NEXT_DATA__ via deep extraction! (length: \(nextData.count))")
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID
                if nextData.contains("00000000-3c8c-6284-bc03-c61add5e47f1") {
                    req.logger.info("‚úÖ Found correct UUID in __NEXT_DATA__!")
                }
            } else {
                req.logger.error("‚ùå __NEXT_DATA__ not found even with deep extraction - this is the core problem!")
            }
        }
        
        return html
    } catch {
        req.logger.error("ScraperAPI request failed: \(error)")
        throw Abort(.badRequest, reason: "ScraperAPI request failed: \(error.localizedDescription)")
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ Crawlbase (–ë–ï–°–ü–õ–ê–¢–ù–û: 1000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü!)
private func fetchViaCrawlbase(url: String, apiKey: String, req: Request) async throws -> String {
    // Crawlbase –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –∏ –æ–±—Ö–æ–¥ Cloudflare
    // –ë–µ—Å–ø–ª–∞—Ç–Ω—ã–π –ø–ª–∞–Ω: 1000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü
    let encodedUrl = url.addingPercentEncoding(withAllowedCharacters: .urlQueryAllowed) ?? url
    // js=true –≤–∫–ª—é—á–∞–µ—Ç JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ (–≤–∞–∂–Ω–æ –¥–ª—è __NEXT_DATA__)
    // wait —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –¥–æ 40 —Å–µ–∫—É–Ω–¥ –¥–ª—è –Ω–∞–¥—ë–∂–Ω–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ __NEXT_DATA__
    // page_wait –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –¥–ª—è —Å—Ç—Ä–∞–Ω–∏—Ü —Å –¥–æ–ª–≥–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π JS
    let apiUrl = "https://api.crawlbase.com/?token=\(apiKey)&url=\(encodedUrl)&js=true&wait=40000&page_wait=5000"
    
    req.logger.debug("Crawlbase URL: \(apiUrl.prefix(200))...")
    
    let client = req.client
    let uri = URI(string: apiUrl)
    
    do {
        let response = try await client.get(uri).get()
        
        guard response.status == HTTPStatus.ok else {
            let bodyStr = response.body?.getString(at: 0, length: response.body?.readableBytes ?? 0, encoding: .utf8) ?? ""
            req.logger.error("Crawlbase returned status \(response.status.code): \(bodyStr.prefix(200))")
            throw Abort(.badRequest, reason: "Crawlbase returned status \(response.status.code)")
        }
        
        guard let body = response.body else {
            throw Abort(.badRequest, reason: "Crawlbase returned empty body")
        }
        
        guard let html = body.getString(at: 0, length: body.readableBytes, encoding: .utf8), !html.isEmpty else {
            throw Abort(.badRequest, reason: "Crawlbase returned empty HTML")
        }
        
        req.logger.info("Crawlbase returned HTML (length: \(html.count))")
        
        // –ë–æ–ª–µ–µ –≥–ª—É–±–æ–∫–∏–π –ø–æ–∏—Å–∫ __NEXT_DATA__ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö (–∫–∞–∫ –≤ ScraperAPI)
        let hasNextData = html.contains("__NEXT_DATA__") || 
                         html.contains("__next_data__") || 
                         html.contains("__NEXT_DATA") ||
                         html.contains("NEXT_DATA") ||
                         html.contains("%5B%5B__NEXT_DATA__%5D%5D") ||
                         html.contains("&#x5f;&#x5f;NEXT_DATA&#x5f;&#x5f;")
        
        req.logger.info("Crawlbase HTML contains __NEXT_DATA__ (any format): \(hasNextData)")
        
        // –ï—Å–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω, –ø—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å —á–µ—Ä–µ–∑ —Ñ—É–Ω–∫—Ü–∏—é extractNextDataJSON
        if !hasNextData {
            req.logger.warning("‚ö†Ô∏è Crawlbase: __NEXT_DATA__ not found with simple contains check, trying deep extraction...")
            if let nextData = extractNextDataJSON(from: html) {
                req.logger.info("‚úÖ Crawlbase found __NEXT_DATA__ via deep extraction! (length: \(nextData.count))")
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —Å–æ–¥–µ—Ä–∂–∏—Ç –ª–∏ –æ–Ω –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID
                if nextData.contains("00000000-3c8c-6284-bc03-c61add5e47f1") {
                    req.logger.info("‚úÖ Found correct UUID in Crawlbase __NEXT_DATA__!")
                }
            } else {
                req.logger.error("‚ùå Crawlbase: __NEXT_DATA__ not found even with deep extraction - JS may not have loaded!")
            }
        }
        
        return html
    } catch {
        req.logger.error("Crawlbase request failed: \(error)")
        throw Abort(.badRequest, reason: "Crawlbase request failed: \(error.localizedDescription)")
    }
}

/// –ü–æ–ª—É—á–∞–µ—Ç HTML —á–µ—Ä–µ–∑ Browserless.io API (–Ω–∞–¥—ë–∂–Ω—ã–π JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__)
private func fetchViaBrowserless(url: String, apiKey: String, req: Request) async throws -> String {
    // Browserless.io –∏—Å–ø–æ–ª—å–∑—É–µ—Ç Chrome headless –±—Ä–∞—É–∑–µ—Ä –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–æ–≤—ã–π production endpoint –≤–º–µ—Å—Ç–æ —É—Å—Ç–∞—Ä–µ–≤—à–µ–≥–æ chrome.browserless.io
    let apiUrl = "https://production-sfo.browserless.io/content?token=\(apiKey)"
    
    req.logger.debug("Browserless API URL: \(apiUrl)")
    
    // –§–æ—Ä–º–∏—Ä—É–µ–º JSON body –¥–ª—è Browserless
    // Browserless content API –ø—Ä–∏–Ω–∏–º–∞–µ—Ç —Ç–æ–ª—å–∫–æ url –∏ cookies (–ë–ï–ó options - –æ–Ω–∏ –≤—ã–∑—ã–≤–∞—é—Ç –æ—à–∏–±–∫—É 400)
    var requestBody: [String: Any] = [
        "url": url
    ]
    
    // –í–ê–ñ–ù–û: Browserless content API –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç options!
    // –î–ª—è –æ–∂–∏–¥–∞–Ω–∏—è –∑–∞–≥—Ä—É–∑–∫–∏ JS –Ω—É–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π endpoint –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –Ω–∞–¥–µ—è—Ç—å—Å—è,
    // —á—Ç–æ Browserless –ø–æ–¥–æ–∂–¥—ë—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –≤—Ä–µ–º–µ–Ω–∏ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
    
    // –î–æ–±–∞–≤–ª—è–µ–º –∫—É–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å (–¥–ª—è –æ–±—Ö–æ–¥–∞ Cloudflare)
    if let soraCookies = Environment.get("SORA_COOKIES"), !soraCookies.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
        var cookieArray: [[String: String]] = []
        // –ü–∞—Ä—Å–∏–º –∫—É–∫–∏ –∏–∑ —Å—Ç—Ä–æ–∫–∏ —Ñ–æ—Ä–º–∞—Ç–∞ "cookie1=value1; cookie2=value2"
        let cookiePairs = soraCookies.split(separator: ";").map { $0.trimmingCharacters(in: .whitespaces) }
        for pair in cookiePairs {
            let parts = pair.split(separator: "=", maxSplits: 1)
            if parts.count == 2 {
                cookieArray.append([
                    "name": String(parts[0]),
                    "value": String(parts[1]),
                    "domain": ".sora.chatgpt.com"
                ])
            }
        }
        if !cookieArray.isEmpty {
            requestBody["cookies"] = cookieArray
            req.logger.info("Using SORA_COOKIES with Browserless (\(cookieArray.count) cookies)")
        }
    }
    
    guard let jsonData = try? JSONSerialization.data(withJSONObject: requestBody) else {
        throw Abort(.badRequest, reason: "Failed to encode Browserless request body")
    }
    
    let client = req.client
    let uri = URI(string: apiUrl)
    
    var headers = HTTPHeaders()
    headers.add(name: "Content-Type", value: "application/json")
    
    do {
        let response = try await client.post(uri, headers: headers) { req in
            req.body = ByteBuffer(data: jsonData)
        }.get()
        
        guard response.status == HTTPStatus.ok else {
            let bodyStr = response.body?.getString(at: 0, length: response.body?.readableBytes ?? 0, encoding: .utf8) ?? ""
            req.logger.error("Browserless API returned status \(response.status.code): \(bodyStr.prefix(200))")
            throw Abort(.badRequest, reason: "Browserless API returned status \(response.status.code)")
        }
        
        guard let body = response.body else {
            throw Abort(.badRequest, reason: "Browserless returned empty body")
        }
        
        guard let html = body.getString(at: 0, length: body.readableBytes, encoding: .utf8), !html.isEmpty else {
            throw Abort(.badRequest, reason: "Browserless returned empty HTML")
        }
        
        req.logger.info("Browserless returned HTML (length: \(html.count))")
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –ª–∏ Cloudflare
        if html.contains("Just a moment") || html.contains("cf-browser-verification") || html.contains("Checking your browser") {
            req.logger.warning("‚ö†Ô∏è Browserless returned Cloudflare challenge (HTML length: \(html.count), first 200 chars: \(html.prefix(200)))")
            throw Abort(.badRequest, reason: "Browserless returned Cloudflare challenge")
        }
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ __NEXT_DATA__ –≤ HTML
        let hasNextData = html.contains("__NEXT_DATA__")
        req.logger.info("Browserless HTML contains __NEXT_DATA__: \(hasNextData)")
        if !hasNextData {
            req.logger.warning("‚ö†Ô∏è __NEXT_DATA__ not found in Browserless HTML - JS rendering may be incomplete")
            req.logger.debug("Browserless HTML preview (first 500 chars): \(html.prefix(500))")
        }
        
        return html
    } catch {
        req.logger.error("Browserless request failed: \(error)")
        throw Abort(.badRequest, reason: "Browserless request failed: \(error.localizedDescription)")
    }
}

/// –ü–∞—Ä—Å–∏—Ç HTML —Å—Ç—Ä–∞–Ω–∏—Ü—ã Sora –∏ –∏–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä—è–º—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –≤–∏–¥–µ–æ
private func parseSoraHtml(_ html: String, req: Request) throws -> String {
    // –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤—Å–µ UUID –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    let allUUIDs = extractAllUUIDs(from: html)
    if allUUIDs.isEmpty == false {
        req.logger.info("All UUIDs found on page (\(allUUIDs.count) total): \(allUUIDs.prefix(10).joined(separator: ", "))")
    }
    
    // –î–ï–¢–ê–õ–¨–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï: –Ω–∞—Ö–æ–¥–∏–º –í–°–ï —Å—Å—ã–ª–∫–∏ videos.openai.com –≤ HTML –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    let allVideoUrls = extractAllVideoUrls(from: html)
    req.logger.info("üìä Found \(allVideoUrls.count) total videos.openai.com URLs in HTML")
    if allVideoUrls.count > 0 {
        // –ì—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ —Ç–∏–ø–∞–º
        let downloadable = allVideoUrls.filter { $0.contains("downloadable") }
        let encodings = allVideoUrls.filter { $0.contains("encodings") || $0.contains("source") }
        let azFiles = allVideoUrls.filter { $0.contains("/az/files/") }
        let drvs = allVideoUrls.filter { $0.contains("/drvs/") }
        let vgAssets = allVideoUrls.filter { $0.contains("vg-assets") }
        let thumbnails = allVideoUrls.filter { $0.contains("thumbnail") }
        
        req.logger.info("üìä URL breakdown: downloadable=\(downloadable.count), encodings/source=\(encodings.count), /az/files/=\(azFiles.count), /drvs/=\(drvs.count), vg-assets=\(vgAssets.count), thumbnails=\(thumbnails.count)")
        
        // –õ–æ–≥–∏—Ä—É–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (–ø–µ—Ä–≤—ã–µ 10)
        let uniqueUrls = Array(Set(allVideoUrls)).prefix(10)
        for (idx, url) in uniqueUrls.enumerated() {
            let type = url.contains("/drvs/md/raw") ? "‚ö†Ô∏è WATERMARKED" :
                       url.contains("/az/files/") && !url.contains("/drvs/") ? "‚úÖ POTENTIAL NO-WATERMARK" :
                       url.contains("downloadable") ? "üéØ DOWNLOADABLE" :
                       url.contains("encodings") || url.contains("source") ? "üéØ ENCODINGS" :
                       url.contains("thumbnail") ? "üñºÔ∏è THUMBNAIL" : "üìπ OTHER"
            req.logger.debug("  [\(idx)] \(type): \(url.prefix(200))...")
        }
    }
    
    // –í–ê–ñ–ù–û: —Å–Ω–∞—á–∞–ª–∞ –ø–∞—Ä—Å–∏–º JSON (__NEXT_DATA__), —Ç–∞–º –º–æ–≥—É—Ç –±—ã—Ç—å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏
    let nextJsonOpt = extractNextDataJSON(from: html)
    let hasNextData = nextJsonOpt != nil
    req.logger.debug("Sora __NEXT_DATA__ found=\(hasNextData)")
    if let nextJson = nextJsonOpt {
        req.logger.debug("Sora __NEXT_DATA__ size=\(nextJson.count)")
        // –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –≤ JSON –Ω–∞–ø—Ä—è–º—É—é (–º–æ–∂–µ—Ç –±—ã—Ç—å JSON-escaped URL)
        if let found = extractDirectUrl(from: nextJson, logger: req.logger) { 
            req.logger.info("‚úÖ Found URL in JSON (direct search) - this should be original without watermark: \(found)")
            return found 
        }
        // –ó–∞—Ç–µ–º —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–∞—Ä—Å–∏–º JSON —Å—Ç—Ä—É–∫—Ç—É—Ä—É (downloadable_url, encodings.source.path)
        if let fromParsed = extractFromNextData(nextJson, logger: req.logger) { 
            req.logger.info("‚úÖ Found URL in JSON (parsed structure: downloadable_url/encodings.source.path) - this should be original without watermark: \(fromParsed)")
            return fromParsed 
        }
        req.logger.warning("‚ö†Ô∏è __NEXT_DATA__ found but no downloadable_url or encodings.source.path in it")
    } else {
        req.logger.info("‚ÑπÔ∏è __NEXT_DATA__ not found - will try to use /az/files/{uuid}/raw if UUID matches main video (like nosorawm.app does, should be original without watermark)")
    }
    
    // –ò—â–µ–º downloadable_url –∏ encodings.source.path –Ω–∞–ø—Ä—è–º—É—é –≤ HTML (–¥–∞–∂–µ –µ—Å–ª–∏ __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω)
    // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∏ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
    
    // 1. downloadable_url - —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
    // –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: downloadable_url —Å–æ–¥–µ—Ä–∂–∏—Ç –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!
    let downloadablePatterns = [
        #""downloadable_url"\s*:\s*"([^"]+)"#,  // –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π JSON
        #"downloadable_url["\s]*:[\s]*["']([^"']+)["']"#,  // –° –æ–¥–∏–Ω–∞—Ä–Ω—ã–º–∏ –∫–∞–≤—ã—á–∫–∞–º–∏
        #"downloadableUrl["\s]*:[\s]*["']([^"']+)["']"#,  // camelCase
        #"downloadable_url["\s]*:[\s]*([^\s,}]+)"#,  // –ë–µ–∑ –∫–∞–≤—ã—á–µ–∫
        #"downloadable_url%22%3A%22([^%]+)"#,  // Percent-encoded
        #"downloadable_url["\s]*:[\s]*(&quot;|%22)([^&"]+)(&quot;|%22)"#,  // HTML entities
        #""downloadableUrl"\s*:\s*"([^"]+)"#,  // camelCase –≤ –∫–∞–≤—ã—á–∫–∞—Ö
        #"'downloadable_url'\s*:\s*'([^']+)'"#,  // –û–¥–∏–Ω–∞—Ä–Ω—ã–µ –∫–∞–≤—ã—á–∫–∏ –≤–µ–∑–¥–µ
    ]
    
    for pattern in downloadablePatterns {
        if let regex = try? NSRegularExpression(pattern: pattern, options: []),
           let match = regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)),
           match.numberOfRanges > 1,
           let range = Range(match.range(at: 1), in: html) {
            let url = String(html[range])
            let decoded = decodePotentiallyEncodedURL(url)
            if decoded.contains("videos.openai.com") && !decoded.contains("sora.chatgpt.com") && !decoded.contains("thumbnail") {
                req.logger.info("‚úÖ Found downloadable_url in HTML (pattern: \(pattern.prefix(30))...): \(decoded.prefix(150))...")
                return decoded
            }
        }
    }
    
    // 2. encodings.source.path - —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    let encodingPatterns = [
        #"encodings["\s]*:[\s\S]*?"source"["\s]*:[\s\S]*?"path"["\s]*:\s*"([^"]+)"#,  // –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π
        #"encodings\.source\.path["\s]*:[\s]*["']([^"']+)["']"#,  // –¢–æ—á–µ—á–Ω–∞—è –Ω–æ—Ç–∞—Ü–∏—è
        #"encodings["\s]*\{[\s\S]*?source["\s]*\{[\s\S]*?path["\s]*:[\s]*["']([^"']+)["']"#,  // –í–ª–æ–∂–µ–Ω–Ω—ã–µ –æ–±—ä–µ–∫—Ç—ã
        #"source["\s]*:[\s\S]*?path["\s]*:[\s]*["']([^"']+)["']"#,  // –ë–µ–∑ encodings
    ]
    
    for pattern in encodingPatterns {
        if let regex = try? NSRegularExpression(pattern: pattern, options: [.dotMatchesLineSeparators]),
           let match = regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)),
           match.numberOfRanges > 1,
           let range = Range(match.range(at: 1), in: html) {
            let url = String(html[range])
            let decoded = decodePotentiallyEncodedURL(url)
            if decoded.contains("videos.openai.com") && !decoded.contains("sora.chatgpt.com") && !decoded.contains("thumbnail") && !decoded.contains("source_wm") {
                req.logger.info("‚úÖ Found encodings.source.path in HTML (pattern: \(pattern.prefix(30))...): \(decoded.prefix(150))...")
                return decoded
            }
        }
    }
    
    // 3. –ò—â–µ–º window.__NEXT_DATA__, self.__next_f –∏ –¥—Ä—É–≥–∏–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
    let nextDataPatterns = [
        #"window\.__NEXT_DATA__\s*=\s*({[^<]+})"#,
        #"self\.__next_f\s*=\s*\[([^\]]+)\]"#,
        #"__NEXT_DATA__\s*=\s*({[^<]+})"#,
        #"window\.__INITIAL_STATE__\s*=\s*({[^<]+})"#,
    ]
    
    for pattern in nextDataPatterns {
        if let regex = try? NSRegularExpression(pattern: pattern, options: [.dotMatchesLineSeparators]),
           let match = regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)),
           match.numberOfRanges > 1,
           let range = Range(match.range(at: 1), in: html) {
            let jsonStr = String(html[range])
            req.logger.debug("Found alternative JSON pattern: \(pattern.prefix(30))..., size=\(jsonStr.count)")
            // –ü—Ä–æ–±—É–µ–º –∏–∑–≤–ª–µ—á—å –∏–∑ —ç—Ç–æ–≥–æ JSON
            if let found = extractDirectUrl(from: jsonStr, logger: req.logger) {
                req.logger.info("‚úÖ Found URL in alternative JSON pattern: \(found.prefix(150))...")
                return found
            }
            if let fromParsed = extractFromNextData(jsonStr, logger: req.logger) {
                req.logger.info("‚úÖ Found URL in alternative JSON (parsed): \(fromParsed.prefix(150))...")
                return fromParsed
            }
        }
    }
    
    // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫: –∏—â–µ–º —Å—Å—ã–ª–∫–∏ —á–µ—Ä–µ–∑ –∞–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö URL
    // –ú–æ–∂–µ—Ç –±—ã—Ç—å, –µ—Å—Ç—å —Ä–∞–±–æ—á–∞—è —Å—Å—ã–ª–∫–∞ –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏, –∫–æ—Ç–æ—Ä—É—é –º—ã –ø—Ä–æ–ø—É—Å—Ç–∏–ª–∏
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–∂–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ allVideoUrls (–æ–±—ä—è–≤–ª–µ–Ω—ã –≤—ã—à–µ)
    
    // –ò—â–µ–º —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –±—ã—Ç—å –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏:
    // 1. /az/files/{uuid}/raw –±–µ–∑ /drvs/ (—É–∂–µ –∏—Å–∫–∞–ª–∏ –≤—ã—à–µ, –Ω–æ –ø—Ä–æ–≤–µ—Ä–∏–º –µ—â—ë —Ä–∞–∑)
    // 2. vg-assets —Å src.mp4 (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç)
    // 3. –õ—é–±—ã–µ —Å—Å—ã–ª–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –Ω–µ —Å–æ–¥–µ—Ä–∂–∞—Ç /drvs/ –∏ thumbnail
    let potentialNoWatermark = allVideoUrls.filter { url in
        url.contains("videos.openai.com") &&
        !url.contains("sora.chatgpt.com") &&
        !url.contains("thumbnail") &&
        !url.contains(".jpeg") &&
        !url.contains(".jpg") &&
        !url.contains(".png") &&
        !url.contains("/drvs/") &&
        (url.contains("/az/files/") || url.contains("vg-assets"))
    }
    
    if potentialNoWatermark.isEmpty == false {
        // –ù–∞—Ö–æ–¥–∏–º UUID –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ /drvs/md/raw –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏
        let mainVideoUUIDs = Set(allVideoUrls.compactMap { url in
            if url.contains("/drvs/md/raw") {
                return extractUUIDFromDrvsUrl(url)
            }
            return nil
        })
        
        // –í–ê–ñ–ù–û: —Å—Å—ã–ª–∫–∏ —Å UUID, –æ—Ç–ª–∏—á–Ω—ã–º –æ—Ç –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ, –º–æ–≥—É—Ç –±—ã—Ç—å –õ–Æ–ë–´–ú –º–µ–¥–∏–∞ (–∞–≤–∞—Ç–∞—Ä–∫–∞, –¥—Ä—É–≥–æ–µ –≤–∏–¥–µ–æ, –∏ —Ç.–¥.)
        // –ë–µ–∑ __NEXT_DATA__ –º—ã –Ω–µ –º–æ–∂–µ–º –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Å—ã–ª–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏ –∏–ª–∏ –ø—Ä–æ—Å—Ç–æ –¥—Ä—É–≥–∏–º –º–µ–¥–∏–∞.
        // –í–ê–ñ–ù–û: UUID –≤ /az/files/{uuid}/raw –ú–û–ñ–ï–¢ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç UUID –≤ /drvs/md/raw!
        // –≠—Ç–æ –ø–æ—Ç–æ–º—É —á—Ç–æ /drvs/md/raw - —ç—Ç–æ –≤–µ—Ä—Å–∏—è –° –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –∞ /az/files/{uuid}/raw - –æ—Ä–∏–≥–∏–Ω–∞–ª –ë–ï–ó –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏.
        // –ü–æ—ç—Ç–æ–º—É –∏—Å–ø–æ–ª—å–∑—É–µ–º –ª—é–±—É—é /az/files/{uuid}/raw —Å—Å—ã–ª–∫—É —Å –ø–æ–ª–Ω—ã–º–∏ SAS –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏ (–∫–∞–∫ nosorawm.app)
        for url in potentialNoWatermark {
            if url.contains("/az/files/") && url.contains("/raw") && !url.contains("/drvs/") {
                let uuid = extractUUIDFromDirectRaw(url) ?? ""
                // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –µ—Å—Ç—å –≤—Å–µ SAS –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (—ç—Ç–æ –≤–∞–∂–Ω–æ –¥–ª—è —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ —Å—Å—ã–ª–∫–∏)
                let hasAllParams = (url.contains("?se=") || url.contains("&se=")) && 
                                   (url.contains("?sp=") || url.contains("&sp=")) && 
                                   (url.contains("?sig=") || url.contains("&sig=")) && 
                                   (url.contains("?ac=") || url.contains("&ac="))
                if hasAllParams {
                    // –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å /drvs/md/raw, –∏–º–µ–µ—Ç –í–´–°–®–ò–ô –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!
                    // –≠—Ç–æ –ø–æ—Ç–æ–º—É —á—Ç–æ /drvs/md/raw - —ç—Ç–æ –≤–µ—Ä—Å–∏—è –° –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –∞ –æ—Ä–∏–≥–∏–Ω–∞–ª –ë–ï–ó –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏ –∏–º–µ–µ—Ç –î–†–£–ì–û–ô UUID!
                    // –ü–æ—ç—Ç–æ–º—É —Å–Ω–∞—á–∞–ª–∞ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Å—ã–ª–∫–∏ —Å UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å main video UUID
                    let isMainVideo = !uuid.isEmpty && mainVideoUUIDs.contains(uuid)
                    if !isMainVideo {
                        req.logger.info("‚úÖ Found /az/files/{uuid}/raw with UUID \(uuid) (NOT matching main video UUID - this is GOOD! /drvs/md/raw has watermark, this should be original without watermark). SAS params present. Using it (like nosorawm.app)!")
                        return url
                    } else {
                        req.logger.debug("Found /az/files/{uuid}/raw with UUID \(uuid) matching main video (has watermark), continuing to search for original...")
                        // –ù–µ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º —Å—Ä–∞–∑—É - –ø—Ä–æ–¥–æ–ª–∂–∞–µ–º –ø–æ–∏—Å–∫, –º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–∞–π–¥–µ–Ω –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏
                        continue
                    }
                } else {
                    req.logger.debug("Found /az/files/{uuid}/raw but missing SAS params, continuing search")
                    continue
                }
            } else if url.contains("vg-assets") {
                // vg-assets —Å—Å—ã–ª–∫–∏ –º–æ–∂–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å, –µ—Å–ª–∏ –æ–Ω–∏ –Ω–µ thumbnail
                if !url.contains("thumbnail") && !url.contains(".jpeg") && !url.contains(".jpg") && !url.contains(".png") {
                    req.logger.info("Found potential no-watermark URL (vg-assets): \(url.prefix(150))...")
                    return url
                }
            }
        }
        
        // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ–¥—Ö–æ–¥—è—â—É—é —Å—Å—ã–ª–∫—É, –∏—Å–ø–æ–ª—å–∑—É–µ–º /drvs/md/raw (–≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç, —Ö–æ—Ç—è —Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π)
        req.logger.info("No suitable no-watermark URL found, will use /drvs/md/raw (has watermark but guaranteed to work)")
    }
    
    // –í–ê–ñ–ù–û: vg-assets –ù–ï —Ä–∞–±–æ—Ç–∞—é—Ç –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!
    // –≠—Ç–∞–ª–æ–Ω–Ω–∞—è —Å—Å—ã–ª–∫–∞ –æ—Ç nosorawm.app - —ç—Ç–æ /az/files/{uuid}/raw —Å –ø–æ–ª–Ω—ã–º–∏ SAS –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
    // –ü–æ—ç—Ç–æ–º—É –º—ã –ù–ï –∏—Å–ø–æ–ª—å–∑—É–µ–º vg-assets, –∞ –∏—â–µ–º —Ç–æ–ª—å–∫–æ /az/files/{uuid}/raw
    // vg-assets –æ—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω–∏–π fallback, –µ—Å–ª–∏ –≤–æ–æ–±—â–µ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ
    
    // Fallback: –∏—â–µ–º –≤ HTML –Ω–∞–ø—Ä—è–º—É—é
    if let found = extractDirectUrl(from: html, logger: req.logger) { return found }
    
    // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ task_id
    if let taskId = extractTaskId(from: html) {
        req.logger.info("Found task_id: \(taskId)")
        req.logger.debug("Found task_id but cannot construct vg-assets URL without SAS params")
    }
    
    let hasHost = html.contains("videos.openai.com")
    req.logger.debug("Sora HTML len=\(html.count) hasVideosHost=\(hasHost)")
    throw Abort(.notFound)
}

private func fetchDirectSoraVideoUrl(from shareUrl: String, req: Request) async throws -> String {
    req.logger.info("üîç Starting fetchDirectSoraVideoUrl for URL: \(shareUrl)")
    // –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ —Å–µ—Ä–≤–∏—Å—ã –¥–ª—è —Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–∞ JS –≤ –ø–æ—Ä—è–¥–∫–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
    // –í–ê–ñ–ù–û: –î–æ–±–∞–≤–ª—è–µ–º retry –ª–æ–≥–∏–∫—É - –∏–Ω–æ–≥–¥–∞ –Ω—É–∂–Ω–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–ø—ã—Ç–æ–∫ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__
    
    // 1. Playwright-—Å–µ—Ä–≤–∏—Å (–ª–æ–∫–∞–ª—å–Ω—ã–π Docker-–∫–æ–Ω—Ç–µ–π–Ω–µ—Ä) - –ü–†–ò–û–†–ò–¢–ï–¢ #1!
    // –ò—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –±—Ä–∞—É–∑–µ—Ä, –¥–æ–ª–∂–µ–Ω –Ω–∞–¥—ë–∂–Ω–æ –ø–æ–ª—É—á–∞—Ç—å __NEXT_DATA__
    let playwrightServiceUrl = Environment.get("PLAYWRIGHT_SERVICE_URL") ?? "http://localhost:3000"
    req.logger.info("üé≠ Trying Playwright service first (real browser, should get __NEXT_DATA__ reliably)...")
    
    do {
        let html = try await fetchViaPlaywright(url: shareUrl, serviceUrl: playwrightServiceUrl, req: req)
        var hasNextData = html.contains("__NEXT_DATA__") || 
                         html.contains("__next_data__") || 
                         html.contains("__NEXT_DATA") ||
                         html.contains("NEXT_DATA")
        
        // –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –≥–ª—É–±–æ–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
        if !hasNextData {
            if let _ = extractNextDataJSON(from: html) {
                hasNextData = true
                req.logger.info("‚úÖ Playwright found __NEXT_DATA__ via deep extraction!")
            }
        }
        
        if !html.contains("Just a moment") && !html.contains("cf-browser-verification") {
            req.logger.info("Playwright success, parsing HTML...")
            do {
                let result = try parseSoraHtml(html, req: req)
                if hasNextData {
                    req.logger.info("‚úÖ Playwright found URL with __NEXT_DATA__ (should be original without watermark): \(result.prefix(150))...")
                } else {
                    req.logger.warning("‚ö†Ô∏è Playwright found URL but WITHOUT __NEXT_DATA__: \(result.prefix(150))...")
                }
                return result
            } catch {
                req.logger.warning("Playwright parsing failed: \(error)")
                // –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫ fallback –º–µ—Ç–æ–¥–∞–º
            }
        } else {
            req.logger.warning("Playwright returned Cloudflare challenge, trying alternatives...")
        }
    } catch {
        req.logger.warning("‚ö†Ô∏è Playwright service failed: \(error.localizedDescription) - trying alternatives...")
        // –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∫ fallback –º–µ—Ç–æ–¥–∞–º
    }
    
    // 2. Browserless.io API - –û–¢–ö–õ–Æ–ß–ï–ù: –≤—Å–µ–≥–¥–∞ –±–ª–æ–∫–∏—Ä—É–µ—Ç—Å—è Cloudflare, –¥–∞–∂–µ —Å –∫—É–∫–∞–º–∏
    // –û—Å—Ç–∞–≤–ª—è–µ–º –∫–æ–¥ –∑–∞–∫–æ–º–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–º –Ω–∞ —Å–ª—É—á–∞–π, –µ—Å–ª–∏ —Å–∏—Ç—É–∞—Ü–∏—è –∏–∑–º–µ–Ω–∏—Ç—Å—è
    /*
    if let apiKey = Environment.get("BROWSERLESS_API_KEY"), !apiKey.isEmpty {
        req.logger.info("Trying Browserless.io API first...")
        
        // –ü—Ä–æ–±—É–µ–º –¥–æ 3 —Ä–∞–∑ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__
        var lastError: Error?
        for attempt in 1...3 {
            req.logger.info("Browserless attempt \(attempt)/3...")
            
            if let html = try? await fetchViaBrowserless(url: shareUrl, apiKey: apiKey, req: req) {
                if !html.contains("Just a moment") && !html.contains("cf-browser-verification") {
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ __NEXT_DATA__ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ (—Ä–∞–∑–Ω—ã–µ —Ñ–æ—Ä–º–∞—Ç—ã)
                    let hasNextData = html.contains("__NEXT_DATA__") || 
                                     html.contains("\"__NEXT_DATA__\"") || 
                                     html.contains("'__NEXT_DATA__'") ||
                                     html.range(of: "__NEXT_DATA__", options: .caseInsensitive) != nil
                    
                    if hasNextData {
                        req.logger.info("‚úÖ Browserless found __NEXT_DATA__ on attempt \(attempt)!")
                    } else {
                        req.logger.warning("‚ö†Ô∏è Browserless attempt \(attempt): __NEXT_DATA__ not found (HTML length: \(html.count))")
                        // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
                        let decoded = html.removingPercentEncoding ?? html
                        if decoded.contains("__NEXT_DATA__") {
                            req.logger.info("‚úÖ Found __NEXT_DATA__ in decoded HTML!")
                        }
                    }
                    
                    req.logger.info("Browserless success, parsing HTML...")
                    do {
                        let result = try parseSoraHtml(html, req: req)
                        // –ï—Å–ª–∏ Browserless –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É –∏ –µ—Å—Ç—å __NEXT_DATA__, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!
                        if hasNextData {
                            req.logger.info("‚úÖ Browserless found URL with __NEXT_DATA__ (may be original without watermark): \(result.prefix(150))...")
                        } else {
                            req.logger.info("‚úÖ Browserless found URL (attempt \(attempt)): \(result.prefix(150))...")
                        }
                        return result
                    } catch {
                        // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç __NEXT_DATA__
                        if !hasNextData && attempt < 3 {
                            req.logger.warning("Browserless attempt \(attempt) returned HTML without __NEXT_DATA__ and no URL found, retrying...")
                            // –ñ–¥—ë–º –Ω–µ–º–Ω–æ–≥–æ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø–æ–ø—ã—Ç–∫–æ–π
                            try? await Task.sleep(nanoseconds: 2_000_000_000) // 2 —Å–µ–∫—É–Ω–¥—ã
                            lastError = error
                            continue
                        } else if !hasNextData {
                            req.logger.warning("Browserless returned HTML without __NEXT_DATA__ after \(attempt) attempts, trying alternatives...")
                            lastError = error
                            break
                        } else {
                            // –ï—Å–ª–∏ –µ—Å—Ç—å __NEXT_DATA__ –Ω–æ –Ω–µ –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É - —ç—Ç–æ –æ—à–∏–±–∫–∞
                            throw error
                        }
                    }
                } else {
                    req.logger.warning("Browserless attempt \(attempt) returned Cloudflare challenge")
                    if attempt < 3 {
                        try? await Task.sleep(nanoseconds: 2_000_000_000) // 2 —Å–µ–∫—É–Ω–¥—ã
                        continue
                    }
                }
            } else if attempt < 3 {
                req.logger.warning("Browserless attempt \(attempt) failed, retrying...")
                try? await Task.sleep(nanoseconds: 2_000_000_000) // 2 —Å–µ–∫—É–Ω–¥—ã
            }
        }
        
        if let error = lastError {
            req.logger.warning("Browserless failed after 3 attempts, trying alternatives...")
        }
    }
    */
    
    // 2. ScrapingBee API (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω) - –ü–†–û–ü–£–°–ö–ê–ï–ú –ò–ó-–ó–ê –õ–ò–ú–ò–¢–ê, –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–µ—Å–ø–ª–∞—Ç–Ω—ã–µ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã
    // –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π –µ—Å–ª–∏ –Ω—É–∂–Ω–æ –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å ScrapingBee
    /*
    if let apiKey = Environment.get("SCRAPINGBEE_API_KEY"), !apiKey.isEmpty {
        req.logger.info("Trying ScrapingBee API...")
        
        // –ü—Ä–æ–±—É–µ–º –¥–æ 3 —Ä–∞–∑ –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è __NEXT_DATA__
        var lastError: Error?
        for attempt in 1...3 {
            req.logger.info("ScrapingBee attempt \(attempt)/3...")
            
            if let html = try? await fetchViaScrapingBee(url: shareUrl, apiKey: apiKey, req: req) {
                if !html.contains("Just a moment") && !html.contains("cf-browser-verification") {
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ __NEXT_DATA__ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω–æ
                    let hasNextData = html.contains("__NEXT_DATA__") || 
                                     html.contains("\"__NEXT_DATA__\"") || 
                                     html.range(of: "__NEXT_DATA__", options: .caseInsensitive) != nil
                    
                    if hasNextData {
                        req.logger.info("‚úÖ ScrapingBee found __NEXT_DATA__ on attempt \(attempt)!")
                    } else {
                        req.logger.warning("‚ö†Ô∏è ScrapingBee attempt \(attempt): __NEXT_DATA__ not found (HTML length: \(html.count))")
                        let decoded = html.removingPercentEncoding ?? html
                        if decoded.contains("__NEXT_DATA__") {
                            req.logger.info("‚úÖ Found __NEXT_DATA__ in decoded HTML!")
                        }
                    }
                    
                    req.logger.info("ScrapingBee success, parsing HTML...")
                    do {
                        let result = try parseSoraHtml(html, req: req)
                        // –ï—Å–ª–∏ ScrapingBee –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É –∏ –µ—Å—Ç—å __NEXT_DATA__, —ç—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!
                        if hasNextData {
                            req.logger.info("‚úÖ ScrapingBee found URL with __NEXT_DATA__ (may be original without watermark): \(result.prefix(150))...")
                        } else {
                            req.logger.info("‚úÖ ScrapingBee found URL (attempt \(attempt)): \(result.prefix(150))...")
                        }
                        return result
                    } catch {
                        // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É, –ø—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—ã —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–µ—Ç __NEXT_DATA__
                        if !hasNextData && attempt < 3 {
                            req.logger.warning("ScrapingBee attempt \(attempt) returned HTML without __NEXT_DATA__ and no URL found, retrying...")
                            try? await Task.sleep(nanoseconds: 3_000_000_000) // 3 —Å–µ–∫—É–Ω–¥—ã (ScrapingBee –º–µ–¥–ª–µ–Ω–Ω–µ–µ)
                            lastError = error
                            continue
                        } else if !hasNextData {
                            req.logger.warning("ScrapingBee returned HTML without __NEXT_DATA__ after \(attempt) attempts, trying alternatives...")
                            lastError = error
                            break
                        } else {
                            // –ï—Å–ª–∏ –µ—Å—Ç—å __NEXT_DATA__ –Ω–æ –Ω–µ –Ω–∞—à—ë–ª —Å—Å—ã–ª–∫—É - —ç—Ç–æ –æ—à–∏–±–∫–∞
                            throw error
                        }
                    }
                } else {
                    req.logger.warning("ScrapingBee attempt \(attempt) returned Cloudflare challenge")
                    if attempt < 3 {
                        try? await Task.sleep(nanoseconds: 3_000_000_000) // 3 —Å–µ–∫—É–Ω–¥—ã
                        continue
                    }
                }
            } else if attempt < 3 {
                req.logger.warning("ScrapingBee attempt \(attempt) failed, retrying...")
                try? await Task.sleep(nanoseconds: 3_000_000_000) // 3 —Å–µ–∫—É–Ω–¥—ã
            }
        }
        
        if let error = lastError {
            req.logger.warning("ScrapingBee failed after 3 attempts, trying alternatives...")
        }
    }
    */
    
    // 3. ScraperAPI (–µ—Å–ª–∏ –Ω–∞—Å—Ç—Ä–æ–µ–Ω) - –ë–ï–°–ü–õ–ê–¢–ù–û: 5000 –∑–∞–ø—Ä–æ—Å–æ–≤/–º–µ—Å—è—Ü!
    if let apiKey = Environment.get("SCRAPERAPI_API_KEY"), !apiKey.isEmpty {
        req.logger.info("Trying ScraperAPI (free tier: 5000 requests/month)...")
        var scraperApiHasNextData = false
        var scraperApiResult: String? = nil
        
        if let html = try? await fetchViaScraperAPI(url: shareUrl, apiKey: apiKey, req: req) {
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É __NEXT_DATA__ (–∫–∞–∫ –≤ fetchViaScraperAPI)
            scraperApiHasNextData = html.contains("__NEXT_DATA__") || 
                                    html.contains("__next_data__") || 
                                    html.contains("__NEXT_DATA") ||
                                    html.contains("NEXT_DATA") ||
                                    html.contains("%5B%5B__NEXT_DATA__%5D%5D") ||
                                    html.contains("&#x5f;&#x5f;NEXT_DATA&#x5f;&#x5f;")
            
            // –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –≥–ª—É–±–æ–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
            if !scraperApiHasNextData {
                if let _ = extractNextDataJSON(from: html) {
                    scraperApiHasNextData = true
                    req.logger.info("‚úÖ ScraperAPI found __NEXT_DATA__ via deep extraction!")
                }
            }
            
            if scraperApiHasNextData {
                req.logger.info("‚úÖ ScraperAPI found __NEXT_DATA__!")
            } else {
                req.logger.warning("‚ö†Ô∏è ScraperAPI did NOT find __NEXT_DATA__ - this is the core problem!")
            }
            
            if !html.contains("Just a moment") && !html.contains("cf-browser-verification") {
                req.logger.info("ScraperAPI success, parsing HTML...")
                do {
                    scraperApiResult = try parseSoraHtml(html, req: req)
                    if scraperApiHasNextData {
                        req.logger.info("‚úÖ ScraperAPI found URL with __NEXT_DATA__ (may be original without watermark): \(scraperApiResult!.prefix(150))...")
                    } else {
                        req.logger.warning("‚ö†Ô∏è ScraperAPI found URL but WITHOUT __NEXT_DATA__ (likely watermarked): \(scraperApiResult!.prefix(150))...")
                    }
                } catch {
                    req.logger.warning("ScraperAPI parsing failed: \(error)")
                }
            }
        }
        
        // –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ï—Å–ª–∏ ScraperAPI –Ω–µ –Ω–∞—à—ë–ª __NEXT_DATA__, –ø—Ä–æ–±—É–µ–º Crawlbase –∫–∞–∫ –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤—É
        // –≠—Ç–æ –º–æ–∂–µ—Ç –¥–∞—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID, –∫–æ—Ç–æ—Ä—ã–π –µ—Å—Ç—å —Ç–æ–ª—å–∫–æ –≤ __NEXT_DATA__
        if !scraperApiHasNextData, let crawlbaseKey = Environment.get("CRAWLBASE_API_KEY"), !crawlbaseKey.isEmpty {
            req.logger.info("üîÑ Trying Crawlbase as alternative (ScraperAPI didn't find __NEXT_DATA__ - this is critical for getting correct UUID!)...")
            do {
                let html = try await fetchViaCrawlbase(url: shareUrl, apiKey: crawlbaseKey, req: req)
                // –ò—Å–ø–æ–ª—å–∑—É–µ–º —É–ª—É—á—à–µ–Ω–Ω—É—é –ø—Ä–æ–≤–µ—Ä–∫—É (—É–∂–µ —Å–¥–µ–ª–∞–Ω–∞ –≤ fetchViaCrawlbase, –Ω–æ –¥—É–±–ª–∏—Ä—É–µ–º –¥–ª—è –ª–æ–≥–∏–∫–∏)
                var hasNextData = html.contains("__NEXT_DATA__") || 
                                  html.contains("__next_data__") || 
                                  html.contains("__NEXT_DATA") ||
                                  html.contains("NEXT_DATA") ||
                                  html.contains("%5B%5B__NEXT_DATA__%5D%5D") ||
                                  html.contains("&#x5f;&#x5f;NEXT_DATA&#x5f;&#x5f;")
                
                // –¢–∞–∫–∂–µ –ø—Ä–æ–±—É–µ–º –≥–ª—É–±–æ–∫–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ
                if !hasNextData {
                    if let _ = extractNextDataJSON(from: html) {
                        hasNextData = true
                        req.logger.info("‚úÖ Crawlbase found __NEXT_DATA__ via deep extraction!")
                    }
                }
                
                if hasNextData {
                    req.logger.info("‚úÖ Crawlbase found __NEXT_DATA__! This should give us the correct UUID!")
                }
                
                if !html.contains("Just a moment") && !html.contains("cf-browser-verification") {
                    req.logger.info("Crawlbase success, parsing HTML...")
                    do {
                        let result = try parseSoraHtml(html, req: req)
                        if hasNextData {
                            req.logger.info("‚úÖ Crawlbase found URL with __NEXT_DATA__ (should be original without watermark): \(result.prefix(150))...")
                            return result
                        } else {
                            req.logger.warning("‚ö†Ô∏è Crawlbase found URL but WITHOUT __NEXT_DATA__: \(result.prefix(150))...")
                        }
                    } catch {
                        req.logger.warning("Crawlbase parsing failed: \(error)")
                    }
                }
            } catch {
                // Crawlbase –º–æ–∂–µ—Ç –≤–µ—Ä–Ω—É—Ç—å –æ—à–∏–±–∫—É 520 (Cloudflare –±–ª–æ–∫–∏—Ä—É–µ—Ç), –Ω–æ —ç—Ç–æ –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ
                // –µ—Å–ª–∏ —É –Ω–∞—Å —É–∂–µ –µ—Å—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç ScraperAPI
                req.logger.warning("‚ö†Ô∏è Crawlbase failed: \(error.localizedDescription) - continuing with ScraperAPI result if available")
            }
        } else if !scraperApiHasNextData {
            req.logger.error("‚ùå CRAWLBASE_API_KEY not configured! Cannot get correct UUID without __NEXT_DATA__!")
        }
        
        // –ï—Å–ª–∏ –ø–æ–ª—É—á–∏–ª–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç –æ—Ç ScraperAPI (–¥–∞–∂–µ –±–µ–∑ __NEXT_DATA__), –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
        if let result = scraperApiResult {
            return result
        }
    }
    
    // 5. Fallback: –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–π curl —Å —É–ª—É—á—à–µ–Ω–Ω—ã–º–∏ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏
    // –≠—Ç–æ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - curl –Ω–µ —Ä–µ–Ω–¥–µ—Ä–∏—Ç JS, –ø–æ—ç—Ç–æ–º—É __NEXT_DATA__ –º–æ–∂–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–æ–≤–∞—Ç—å
    req.logger.info("Trying curl as final fallback (no JS rendering, __NEXT_DATA__ may be missing)...")
    
    let process = Process()
    process.executableURL = URL(fileURLWithPath: "/usr/bin/curl")
    
    // –£–ª—É—á—à–µ–Ω–Ω—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è –ª—É—á—à–µ–≥–æ –æ–±—Ö–æ–¥–∞ Cloudflare
    var curlArgs = [
        "-s", "-L", // silent, follow redirects
        "--max-time", "30", // –£–≤–µ–ª–∏—á–∏–ª–∏ —Ç–∞–π–º–∞—É—Ç –¥–æ 30 —Å–µ–∫—É–Ω–¥
        "--user-agent", "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
        "--header", "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7",
        "--header", "Accept-Language: en-US,en;q=0.9",
        "--header", "Accept-Encoding: gzip, deflate, br",
        "--header", "Referer: https://sora.chatgpt.com/",
        "--header", "Origin: https://sora.chatgpt.com",
        "--header", "Cache-Control: no-cache",
        "--header", "Pragma: no-cache",
        "--header", "Sec-Fetch-Dest: document",
        "--header", "Sec-Fetch-Mode: navigate",
        "--header", "Sec-Fetch-Site: same-origin",
        "--header", "Sec-Fetch-User: ?1",
        "--header", "DNT: 1",
        "--header", "Upgrade-Insecure-Requests: 1",
        "--compressed", // –ü–æ–¥–¥–µ—Ä–∂–∫–∞ gzip/deflate
        shareUrl
    ]
    
    // –î–æ–±–∞–≤–ª—è–µ–º –∫—É–∫–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
    if let soraCookies = Environment.get("SORA_COOKIES"), !soraCookies.trimmingCharacters(in: .whitespacesAndNewlines).isEmpty {
        curlArgs.insert("--header", at: curlArgs.count - 1)
        curlArgs.insert("Cookie: \(soraCookies)", at: curlArgs.count - 1)
        req.logger.info("Using SORA_COOKIES with curl")
    }
    
    process.arguments = curlArgs
    
    let pipe = Pipe()
    let errorPipe = Pipe()
    process.standardOutput = pipe
    process.standardError = errorPipe
    
    do {
        try process.run()
        process.waitUntilExit()
        
        guard process.terminationStatus == 0 else {
            let errorData = errorPipe.fileHandleForReading.readDataToEndOfFile()
            let errorMsg = String(data: errorData, encoding: .utf8) ?? "Unknown error"
            req.logger.error("curl failed with status \(process.terminationStatus): \(errorMsg)")
            throw Abort(.badRequest, reason: "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É Sora")
        }
        
        let outputData = pipe.fileHandleForReading.readDataToEndOfFile()
        guard let html = String(data: outputData, encoding: .utf8), !html.isEmpty else {
            req.logger.error("curl returned empty output")
            throw Abort(.badRequest, reason: "–ü—É—Å—Ç–æ–π –æ—Ç–≤–µ—Ç –æ—Ç Sora")
        }
        
        req.logger.debug("curl fetched HTML (length: \(html.count))")
        
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–µ –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–ª –ª–∏ Cloudflare
        if html.contains("Just a moment") || html.contains("cf-browser-verification") {
            req.logger.error("Cloudflare challenge detected in HTML")
            throw Abort(.badRequest, reason: "Cloudflare –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –æ–±–Ω–æ–≤–∏—Ç—å –∫—É–∫–∏ –≤ config/.env –∏–ª–∏ –≤–∫–ª—é—á–∏—Ç—å VPN –Ω–∞ –°–®–ê.")
        }
        
        // –ü–∞—Ä—Å–∏–º HTML
        return try parseSoraHtml(html, req: req)
        
    } catch {
        req.logger.error("Failed to fetch Sora page via curl: \(error)")
        throw Abort(.badRequest, reason: "–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É Sora: \(error.localizedDescription)")
    }
}

private func extractDirectUrl(from html: String, logger: Logger? = nil) -> String? {
    // 1) –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ –≤ —á–∏—Å—Ç–æ–º –≤–∏–¥–µ
    let mp4Pattern = #"https://videos\.openai\.com[^\s"'<>]+?src\.mp4[^\s"'<>]*"#
    if let found = firstMatch(in: html, pattern: mp4Pattern) { return found }
    let hlsPattern = #"https://videos\.openai\.com[^\s"'<>]+?hls\.m3u8[^\s"'<>]*"#
    if let found = firstMatch(in: html, pattern: hlsPattern) { return found }

    // 1b) –í–µ—Ç–∫–∞ vg-assets - —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/
    let vgAssetsPattern = #"https://videos\.openai\.com(/az)?/vg-assets/[^\s"'<>\\]+"#
    if let vg = firstMatch(in: html, pattern: vgAssetsPattern) {
        let decoded = decodePotentiallyEncodedURL(vg)
        if decoded.contains("src.mp4") || decoded.contains("m3u8") { return decoded }
    }

    // 2) JSON-escaped: https:\/\/videos.openai.com..." (–∑–∞—Ö–≤–∞—Ç—ã–≤–∞–µ–º –¥–æ –±–ª–∏–∂–∞–π—à–µ–π –∫–∞–≤—ã—á–∫–∏)
    let jsonEscapedPattern = #"https:\/\/videos\.openai\.com[^"]+"#
    if let rawJsonEscaped = firstMatch(in: html, pattern: jsonEscapedPattern) {
        let decoded = decodePotentiallyEncodedURL(rawJsonEscaped)
        if decoded.contains("src.mp4") || decoded.contains("hls.m3u8") { return decoded }
    }

    // 3) Percent-encoded: https%3A%2F%2Fvideos.openai.com...
    let percentEncodedPattern = #"https%3A%2F%2Fvideos\.openai\.com[^&"'<>\s]+"#
    if let rawPercent = firstMatch(in: html, pattern: percentEncodedPattern) {
        let decoded = decodePotentiallyEncodedURL(rawPercent)
        if decoded.contains("src.mp4") || decoded.contains("hls.m3u8") { return decoded }
    }

    // 3b) Percent-encoded –¥–ª—è vg-assets - —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/
    let vgPercentPattern = #"https%3A%2F%2Fvideos\.openai\.com(%2Faz)?%2Fvg-assets%2F[^&"'<>\s]+"#
    if let rawVG = firstMatch(in: html, pattern: vgPercentPattern) {
        let decoded = decodePotentiallyEncodedURL(rawVG)
        if decoded.contains("src.mp4") || decoded.contains("m3u8") { return decoded }
    }

    // 4) –†–∞–∑—Ä–µ—à–∞–µ–º –æ–±—Ä–∞—Ç–Ω—ã–µ —Å–ª—ç—à–∏ –≤–Ω—É—Ç—Ä–∏ –º–∞—Ç—á–∞ –¥–æ –∫–∞–≤—ã—á–∫–∏ (—à–∏—Ä–æ–∫–∏–π –∑–∞—Ö–≤–∞—Ç)
    let relaxedPattern = #"https://videos\.openai\.com[^"]+"#
    if let rawRelaxed = firstMatch(in: html, pattern: relaxedPattern) {
        let decoded = decodePotentiallyEncodedURL(rawRelaxed)
        if decoded.contains("src.mp4") || decoded.contains("hls.m3u8") { return decoded }
    }

    // 5) OpenGraph meta
    let ogMetaPattern = #"<meta[^>]+property=["']og:video["'][^>]+content=["'](https://videos\.openai\.com[^"']+(?:src\.mp4|hls\.m3u8)[^"']*)["'][^>]*>"#
    if let ogURL = firstCapture(in: html, pattern: ogMetaPattern) {
        return decodePotentiallyEncodedURL(ogURL)
    }

    // 6) –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã —Å–æ —Ö–æ—Å—Ç–∞ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à–∏–π
    // –ò—Å–ø–æ–ª—å–∑—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –¥–ª—è –±–æ–ª–µ–µ –Ω–∞–¥—ë–∂–Ω–æ–≥–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—è URL
    // –í–ê–ñ–ù–û: –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ `)`, `]`, `,` - –æ–Ω–∏ –º–æ–≥—É—Ç –±—ã—Ç—å –≤–Ω—É—Ç—Ä–∏ URL –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
    // –ü–∞—Ç—Ç–µ—Ä–Ω 1: –∫–∞–∫ –≤ extractAllVideoUrls - –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ –ø—Ä–æ–±–µ–ª–∞—Ö, –∫–∞–≤—ã—á–∫–∞—Ö –∏ —Ç–µ–≥–∞—Ö
    let anyUrlPattern1 = #"https://videos\.openai\.com[^\s"'<>]+"#
    // –ü–∞—Ç—Ç–µ—Ä–Ω 2: –¥–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π –∫–∞–≤—ã—á–∫–∏ (–¥–ª—è JSON –≤ –∞—Ç—Ä–∏–±—É—Ç–∞—Ö) - –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –ø–∞—Ç—Ç–µ—Ä–Ω—É 1
    let anyUrlPattern2 = #"https://videos\.openai\.com[^"'\s<>]+"#
    // –ü–∞—Ç—Ç–µ—Ä–Ω 3: JSON-escaped —Å –æ–±—Ä–∞—Ç–Ω—ã–º–∏ —Å–ª—ç—à–∞–º–∏
    let anyUrlPattern3 = #"https:\\?/\\?/videos\.openai\.com[^"\\\s]+"#
    
    var rawCandidatesSet = Set<String>()
    let matches1 = allMatches(in: html, pattern: anyUrlPattern1)
    let matches2 = allMatches(in: html, pattern: anyUrlPattern2)
    let matches3 = allMatches(in: html, pattern: anyUrlPattern3)
    rawCandidatesSet.formUnion(matches1)
    rawCandidatesSet.formUnion(matches2)
    rawCandidatesSet.formUnion(matches3)
    let rawCandidates = Array(rawCandidatesSet)
    
    logger?.debug("URL extraction: pattern1=\(matches1.count), pattern2=\(matches2.count), pattern3=\(matches3.count), unique=\(rawCandidates.count)")
    if rawCandidates.isEmpty == false {
        let decoded = rawCandidates.map { decodePotentiallyEncodedURL($0) }
        // –ò—Å–∫–ª—é—á–∞–µ–º thumbnail, jpeg, png –∏ –¥—Ä—É–≥–∏–µ –Ω–µ-–≤–∏–¥–µ–æ —Ñ–∞–π–ª—ã, –∞ —Ç–∞–∫–∂–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ sora.chatgpt.com
        let filtered = decoded.filter { url in
            !url.contains("thumbnail") && 
            !url.contains(".jpeg") && 
            !url.contains(".jpg") && 
            !url.contains(".png") &&
            !url.contains(".webp") &&
            !url.contains("thumbnail.jpeg") &&
            !url.contains("sora.chatgpt.com") // –ò—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ Sora
        }
        // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: /az/files/{id}/raw (–ë–ï–ó /drvs/ - –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!) > vg-assets/.../src.mp4 > –¥—Ä—É–≥–∏–µ src.mp4 > m3u8 > vg-assets > /drvs/*/raw (–ù–ï md) > /drvs/md/raw (fallback)
        // –ò—â–µ–º –í–°–ï –ø—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏ /az/files/{uuid}/raw (–±–µ–∑ /drvs/) - —Å–Ω–∞—á–∞–ª–∞ –±–µ–∑ —Ñ–∏–ª—å—Ç—Ä–∞ –ø–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º
        let allDirectRawCandidates = filtered.filter { url in
            url.contains("/az/files/") && 
            url.contains("/raw") && 
            !url.contains("/drvs/")
        }
        
        // –õ–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if allDirectRawCandidates.isEmpty == false {
            logger?.info("Found \(allDirectRawCandidates.count) direct /az/files/{id}/raw URLs (all candidates, before filtering)")
            for (idx, url) in allDirectRawCandidates.enumerated() {
                let uuid = extractUUIDFromDirectRaw(url) ?? "unknown"
                let hasSe = url.contains("?se=") || url.contains("&se=")
                let hasSp = url.contains("?sp=") || url.contains("&sp=")
                let hasSv = url.contains("?sv=") || url.contains("&sv=")
                let hasSr = url.contains("?sr=") || url.contains("&sr=")
                let hasSig = url.contains("?sig=") || url.contains("&sig=")
                let hasAc = url.contains("?ac=") || url.contains("&ac=")
                let valid = hasSe && hasSp && hasSv && hasSr && hasSig && hasAc
                logger?.debug("  [\(idx)] UUID=\(uuid) valid=\(valid) | se=\(hasSe) sp=\(hasSp) sv=\(hasSv) sr=\(hasSr) sig=\(hasSig) ac=\(hasAc) | URL: \(url.prefix(250))...")
            }
        }
        
        // –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ, —É –∫–æ—Ç–æ—Ä—ã—Ö –µ—Å—Ç—å —Ö–æ—Ç—è –±—ã –±–∞–∑–æ–≤—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã (sp, sig, ac)
        let allDirectRaw = allDirectRawCandidates.filter { url in
            (url.contains("?sp=") || url.contains("&sp=")) && 
            (url.contains("?sig=") || url.contains("&sig=")) && 
            (url.contains("?ac=") || url.contains("&ac="))
        }
        
        if allDirectRaw.isEmpty == false {
            // –í–ê–ñ–ù–û: /az/files/{uuid}/raw –ë–ï–ó /drvs/ –º–æ–≥—É—Ç –±—ã—Ç—å –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏,
            // –ù–û —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –ø–æ–ª—É—á–µ–Ω—ã –∏–∑ __NEXT_DATA__ —Å downloadable_url –∏–ª–∏ encodings.source.path.
            // –ï—Å–ª–∏ __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω, —ç—Ç–∏ —Å—Å—ã–ª–∫–∏ –ú–û–ì–£–¢ –∏–º–µ—Ç—å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫—É, –Ω–æ –º–æ–≥—É—Ç –±—ã—Ç—å –∏ –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º.
            // –ü–æ—ç—Ç–æ–º—É –ø–æ–ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –∏—Ö, –µ—Å–ª–∏ UUID —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å –æ—Å–Ω–æ–≤–Ω—ã–º –≤–∏–¥–µ–æ.
            let hasNextData = html.contains("__NEXT_DATA__")
            if !hasNextData {
                logger?.info("‚ÑπÔ∏è __NEXT_DATA__ not found in HTML - will try to use /az/files/{uuid}/raw links if UUID matches main video (like nosorawm.app does, should be original without watermark)")
            }
            
            // –ò—Å–ø–æ–ª—å–∑—É–µ–º /az/files/{uuid}/raw –¥–∞–∂–µ –µ—Å–ª–∏ __NEXT_DATA__ –Ω–µ –Ω–∞–π–¥–µ–Ω - –ø—Ä–æ–≤–µ—Ä–∏–º UUID –∏ —Ä–∞–±–æ—Ç–æ—Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å
            if true { // –¢–µ–ø–µ—Ä—å –≤—Å–µ–≥–¥–∞ –ø—Ä–æ–±—É–µ–º, –Ω–µ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ __NEXT_DATA__ –Ω–∞–π–¥–µ–Ω
                logger?.info("Found \(allDirectRaw.count) direct /az/files/{id}/raw URLs (after basic filter: sp, sig, ac). Will try to use them if UUID matches main video (__NEXT_DATA__ found=\(hasNextData))")
                
                // –í–ê–ñ–ù–û: –ù–∞—Ö–æ–¥–∏–º UUID –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ –∏–∑ /drvs/md/raw (—ç—Ç–æ –≥–∞—Ä–∞–Ω—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ –≤–∏–¥–µ–æ —ç—Ç–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã)
                let mainVideoUUIDs = Set(filtered.compactMap { url in
                    if url.contains("/drvs/md/raw") {
                        return extractUUIDFromDrvsUrl(url)
                    }
                    return nil
                })
                logger?.debug("Main video UUIDs from /drvs/md/raw: \(mainVideoUUIDs.joined(separator: ", "))")
                
                // –°–æ—Ä—Ç–∏—Ä—É–µ–º: –í–´–°–®–ò–ô –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - UUID —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 00000000- (–æ–Ω–∏ —á–∞—Å—Ç–æ –≤–µ–¥—É—Ç –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—É –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏, –∫–∞–∫ –≤ nosorawm.app)
                // –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å /drvs/md/raw, –∏–º–µ–µ—Ç –í–´–°–®–ò–ô –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç!
                // –≠—Ç–æ –ø–æ—Ç–æ–º—É —á—Ç–æ /drvs/md/raw - —ç—Ç–æ –≤–µ—Ä—Å–∏—è –° –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –∞ –æ—Ä–∏–≥–∏–Ω–∞–ª –ë–ï–ó –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏ –∏–º–µ–µ—Ç –î–†–£–ì–û–ô UUID!
                let sorted = allDirectRaw.sorted { url1, url2 in
                    let uuid1 = extractUUIDFromDirectRaw(url1) ?? ""
                    let uuid2 = extractUUIDFromDirectRaw(url2) ?? ""
                    
                    // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 1: UUID —Å –ø—Ä–µ—Ñ–∏–∫—Å–æ–º 00000000- (–∫–∞–∫ –≤ —Ä–∞–±–æ—á–µ–π —Å—Å—ã–ª–∫–µ –æ—Ç nosorawm.app)
                    let pref1 = uuid1.hasPrefix("00000000-")
                    let pref2 = uuid2.hasPrefix("00000000-")
                    if pref1 && !pref2 { return true }
                    if !pref1 && pref2 { return false }
                    
                    // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç 2: UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å /drvs/md/raw (—ç—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª –ë–ï–ó –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!)
                    // UUID, –∫–æ—Ç–æ—Ä—ã–π —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å /drvs/md/raw - —ç—Ç–æ –≤–µ—Ä—Å–∏—è –° –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –Ω–µ –Ω—É–∂–Ω–∞!
                    let isMain1 = mainVideoUUIDs.contains(uuid1)
                    let isMain2 = mainVideoUUIDs.contains(uuid2)
                    if !isMain1 && isMain2 { return true }  // uuid1 –ù–ï main - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—à–µ!
                    if isMain1 && !isMain2 { return false } // uuid2 –ù–ï main - –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç –≤—ã—à–µ!
                    
                    return url1 < url2
                }
                
                // –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —Å—Å—ã–ª–∫–∞ —Å–æ–¥–µ—Ä–∂–∏—Ç –≤—Å–µ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã–µ SAS-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
                let validated = sorted.filter { url in
                    // –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è Azure Blob SAS
                    // –ü–µ—Ä–≤—ã–π –ø–∞—Ä–∞–º–µ—Ç—Ä –º–æ–∂–µ—Ç –Ω–∞—á–∏–Ω–∞—Ç—å—Å—è —Å ? –∏–ª–∏ &, –æ—Å—Ç–∞–ª—å–Ω—ã–µ —Å &
                    (url.contains("?se=") || url.contains("&se=")) &&  // Signed Expiry
                    (url.contains("?sp=") || url.contains("&sp=")) &&  // Signed Permissions
                    (url.contains("?sv=") || url.contains("&sv=")) &&  // Signed Version
                    (url.contains("?sr=") || url.contains("&sr=")) &&  // Signed Resource
                    (url.contains("?sig=") || url.contains("&sig=")) && // Signature
                    (url.contains("?ac=") || url.contains("&ac="))     // Account
                }
                
                // –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Å–µ—Ö –≤–∞–ª–∏–¥–Ω—ã—Ö —Å—Å—ã–ª–æ–∫ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                if validated.isEmpty == false {
                    logger?.info("üìã Found \(validated.count) validated /az/files/{uuid}/raw URLs with full SAS params:")
                    for (idx, url) in validated.enumerated() {
                        let uuid = extractUUIDFromDirectRaw(url) ?? "unknown"
                        logger?.info("  [\(idx)] UUID=\(uuid) | URL: \(url.prefix(200))...")
                    }
                }
                
                // –í–ê–ñ–ù–û: nosorawm.app —Ä–∞–±–æ—Ç–∞–µ—Ç –∏–º–µ–Ω–Ω–æ —Ç–∞–∫ - –∏—Å–ø–æ–ª—å–∑—É–µ—Ç /az/files/{uuid}/raw —Å—Å—ã–ª–∫–∏,
                // –∫–æ—Ç–æ—Ä—ã–µ –æ–Ω–∏ –Ω–∞—Ö–æ–¥—è—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ, –¥–∞–∂–µ –±–µ–∑ __NEXT_DATA__. –≠—Ç–∏ —Å—Å—ã–ª–∫–∏ –î–û–õ–ñ–ù–´ —Ä–∞–±–æ—Ç–∞—Ç—å!
                // –ö–õ–Æ–ß–ï–í–û–ô –ò–ù–°–ê–ô–¢: UUID –≤ /az/files/{uuid}/raw –ú–û–ñ–ï–¢ –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç UUID –≤ /drvs/md/raw!
                // –≠—Ç–æ –ø–æ—Ç–æ–º—É —á—Ç–æ /drvs/md/raw - —ç—Ç–æ –≤–µ—Ä—Å–∏—è –° –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –∞ /az/files/{uuid}/raw - –æ—Ä–∏–≥–∏–Ω–∞–ª –ë–ï–ó –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏.
                
                // –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ (downloadable_url, encodings.source.path)
                var preferredUUID: String? = nil
                if hasNextData, let nextJson = extractNextDataJSON(from: html) {
                    // –ò—â–µ–º downloadable_url –∏–ª–∏ encodings.source.path –≤ __NEXT_DATA__
                    if let downloadableUrl = extractDirectUrl(from: nextJson, logger: logger) {
                        preferredUUID = extractUUIDFromDirectRaw(downloadableUrl)
                        logger?.info("üéØ Extracted preferred UUID from downloadable_url in __NEXT_DATA__: \(preferredUUID ?? "none")")
                    }
                    if preferredUUID == nil, let encodingPath = extractFromNextData(nextJson, logger: logger) {
                        preferredUUID = extractUUIDFromDirectRaw(encodingPath)
                        logger?.info("üéØ Extracted preferred UUID from encodings.source.path in __NEXT_DATA__: \(preferredUUID ?? "none")")
                    }
                }
                
                // –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º HTML –Ω–∞–ø—Ä—è–º—É—é –¥–ª—è downloadable_url –∏ encodings.source.path
                if preferredUUID == nil {
                    for pattern in [
                        #""downloadable_url"\s*:\s*"([^"]+)"#,
                        #"encodings["\s]*:[\s\S]*?"source"["\s]*:[\s\S]*?"path"["\s]*:\s*"([^"]+)"#
                    ] {
                        if let regex = try? NSRegularExpression(pattern: pattern, options: [.dotMatchesLineSeparators]),
                           let match = regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)),
                           match.numberOfRanges > 1,
                           let range = Range(match.range(at: 1), in: html) {
                            let url = String(html[range])
                            let decoded = decodePotentiallyEncodedURL(url)
                            if decoded.contains("/az/files/") && decoded.contains("/raw") {
                                preferredUUID = extractUUIDFromDirectRaw(decoded)
                                logger?.info("üéØ Extracted preferred UUID from HTML pattern: \(preferredUUID ?? "none")")
                                break
                            }
                        }
                    }
                }
                
                // –ö–†–ò–¢–ò–ß–ï–°–ö–ò –í–ê–ñ–ù–û: –ï—Å–ª–∏ preferred UUID –Ω–µ –Ω–∞–π–¥–µ–Ω, –∏—â–µ–º –í–°–ï UUID –≤ HTML, –∫–æ—Ç–æ—Ä—ã–µ –ù–ï —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å main UUID
                // –ü—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ –¥–æ–ª–∂–µ–Ω –æ—Ç–ª–∏—á–∞—Ç—å—Å—è –æ—Ç UUID –≤ /drvs/md/raw!
                if preferredUUID == nil {
                    let allUUIDsInHtml = extractAllUUIDs(from: html)
                    let uniqueUUIDs = Array(Set(allUUIDsInHtml))
                    logger?.info("üîç Found \(uniqueUUIDs.count) unique UUIDs in HTML: \(uniqueUUIDs.joined(separator: ", "))")
                    logger?.info("üìã Main video UUIDs (from /drvs/md/raw, have watermark): \(mainVideoUUIDs.joined(separator: ", "))")
                    
                    // –ò—â–µ–º UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å main video UUID –∏ –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å 00000000-
                    // –≠—Ç–æ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π UUID –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–≥–æ –≤–∏–¥–µ–æ!
                    let candidateUUIDs = uniqueUUIDs.filter { uuid in
                        uuid.hasPrefix("00000000-") && !mainVideoUUIDs.contains(uuid)
                    }
                    
                    if candidateUUIDs.isEmpty == false {
                        logger?.info("üéØ Found \(candidateUUIDs.count) candidate UUIDs (00000000- prefix, NOT matching main video - should be original without watermark): \(candidateUUIDs.joined(separator: ", "))")
                        
                        // –ü—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ —Å—Å—ã–ª–∫—É —Å —ç—Ç–∏–º UUID –≤ validated URLs
                        for candidateUUID in candidateUUIDs {
                            if let urlWithUUID = validated.first(where: { extractUUIDFromDirectRaw($0) == candidateUUID }) {
                                logger?.info("‚úÖ FOUND! URL with candidate UUID \(candidateUUID) (NOT matching main video - should be original without watermark): \(urlWithUUID.prefix(200))...")
                                preferredUUID = candidateUUID
                                break
                            }
                        }
                        
                        // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≥–æ—Ç–æ–≤—É—é —Å—Å—ã–ª–∫—É, –Ω–æ –Ω–∞—à–ª–∏ UUID - —ç—Ç–æ –≤–∞–∂–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
                        if preferredUUID == nil {
                            logger?.warning("‚ö†Ô∏è Found candidate UUIDs (\(candidateUUIDs.joined(separator: ", "))) but no corresponding /az/files/{uuid}/raw links with full SAS params found")
                            logger?.warning("‚ö†Ô∏è This means the correct UUID exists in HTML but doesn't have a complete /az/files/{uuid}/raw link with all SAS params")
                        }
                    } else {
                        logger?.warning("‚ö†Ô∏è No candidate UUIDs found! All UUIDs in HTML either match main video UUID or don't have 00000000- prefix")
                        logger?.warning("‚ö†Ô∏è The correct UUID (00000000-3c8c-6284-bc03-c61add5e47f1) is NOT in HTML - it's only in __NEXT_DATA__ which didn't load!")
                    }
                }
                
                if validated.isEmpty == false {
                    // –ï—Å–ª–∏ –µ—Å—Ç—å preferred UUID, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Å—ã–ª–∫—É —Å —ç—Ç–∏–º UUID
                    if let preferredUUID = preferredUUID,
                       let preferredUrl = validated.first(where: { extractUUIDFromDirectRaw($0) == preferredUUID }) {
                        logger?.info("‚úÖ Using /az/files/{uuid}/raw with preferred UUID \(preferredUUID) from downloadable_url/encodings.source.path. This should be original without watermark!")
                        return preferredUrl
                    }
                    
                    // –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å main video UUID (—ç—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!)
                    let nonMainUrls = validated.filter { url in
                        let uuid = extractUUIDFromDirectRaw(url) ?? ""
                        return !mainVideoUUIDs.contains(uuid)
                    }
                    
                    if nonMainUrls.isEmpty == false {
                        // –ù–∞—à–ª–∏ —Å—Å—ã–ª–∫—É —Å UUID, –∫–æ—Ç–æ—Ä—ã–π –ù–ï —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å main - —ç—Ç–æ –æ—Ä–∏–≥–∏–Ω–∞–ª!
                        let bestUrl = nonMainUrls.first!
                        let bestUUID = extractUUIDFromDirectRaw(bestUrl) ?? ""
                        logger?.info("‚úÖ Found /az/files/{uuid}/raw with UUID \(bestUUID) (NOT matching main video UUID - this is GOOD! /drvs/md/raw has watermark, this should be original without watermark). Full SAS params present. Using it (like nosorawm.app does)!")
                        return bestUrl
                    }
                    
                    // –ï—Å–ª–∏ –≤—Å–µ UUID —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å main, –ø—Ä–æ–±—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ü–û–°–õ–ï–î–ù–ò–ô –Ω–∞–π–¥–µ–Ω–Ω—ã–π
                    // (–º–æ–∂–µ—Ç –±—ã—Ç—å –æ–Ω –ø–æ—è–≤–∏–ª—Å—è –ø–æ–∑–∂–µ –≤ HTML –∏ —è–≤–ª—è–µ—Ç—Å—è –æ—Ä–∏–≥–∏–Ω–∞–ª–æ–º, –Ω–æ –µ–≥–æ UUID —Å–ª—É—á–∞–π–Ω–æ —Å–æ–≤–ø–∞–ª)
                    // –ò–ª–∏ –ø—Ä–æ–±—É–µ–º –≤—Å–µ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –ø–æ –æ—á–µ—Ä–µ–¥–∏, –Ω–∞—á–∏–Ω–∞—è —Å –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ
                    if validated.count > 1 {
                        logger?.warning("‚ö†Ô∏è All found UUIDs match main video (likely have watermark). Trying last found URL - may be original despite matching UUID: \(validated.last!.prefix(200))...")
                        return validated.last!
                    }
                    
                    // –ï—Å–ª–∏ —Ç–æ–ª—å–∫–æ –æ–¥–Ω–∞ —Å—Å—ã–ª–∫–∞ –∏ –æ–Ω–∞ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å main - –∏—Å–ø–æ–ª—å–∑—É–µ–º –µ—ë –∫–∞–∫ fallback
                    let bestUrl = validated.first!
                    let bestUUID = extractUUIDFromDirectRaw(bestUrl) ?? ""
                    logger?.warning("‚ö†Ô∏è Found /az/files/{uuid}/raw with UUID \(bestUUID) matching main video (has watermark, but no better option found). Full SAS params present. Using it as fallback.")
                    return bestUrl
                } else {
                    logger?.warning("‚ùå Found \(sorted.count) direct /az/files/{id}/raw URLs but none passed validation (missing SAS params), falling back to /drvs/md/raw")
                }
            }
        }
        
        // –ù–∞—Ö–æ–¥–∏–º UUID –æ—Å–Ω–æ–≤–Ω–æ–≥–æ –≤–∏–¥–µ–æ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫
        let mainVideoUUIDs = Set(filtered.compactMap { url in
            if url.contains("/drvs/md/raw") {
                return extractUUIDFromDrvsUrl(url)
            }
            return nil
        })
        
        // –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≥–æ—Ç–æ–≤—É—é —Å—Å—ã–ª–∫—É —Å –ø–æ–ª–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏, –Ω–µ –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º - —ç—Ç–æ –¥–∞—Å—Ç –æ—à–∏–±–∫—É Signature
        // –í–º–µ—Å—Ç–æ —ç—Ç–æ–≥–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º /drvs/md/raw (—Ö–æ—Ç—è —Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç)
        if let mdRaw = filtered.first(where: { $0.contains("/drvs/md/raw") }) {
            logger?.warning("No direct /az/files/{id}/raw found with full params, returning /drvs/md/raw (has watermark)")
            return mdRaw
        }
        
        // –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –±–æ–ª—å—à–µ –ù–ï –∫–æ–Ω—Å—Ç—Ä—É–∏—Ä—É–µ–º —Å—Å—ã–ª–∫–∏ /az/files/{uuid}/raw –∏–∑ —á–∞—Å—Ç–µ–π,
        // —Ç–∞–∫ –∫–∞–∫ SAS-—Ç–æ–∫–µ–Ω—ã —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—É—Ç–∏ –∏ –ø–µ—Ä–µ–Ω–æ—Å –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–∞—Å—Ç –æ—à–∏–±–∫—É Signature.
        // –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –≥–æ—Ç–æ–≤—ã–µ —Å—Å—ã–ª–∫–∏, –Ω–∞–π–¥–µ–Ω–Ω—ã–µ –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ.
        // –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º UUID –¥–ª—è –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å /az/files/ - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Å—ã–ª–∫–∏ —Å –¥—Ä—É–≥–∏–º UUID
        if let vgAssets = filtered.first(where: { url in
            url.contains("vg-assets") && url.contains("src.mp4") && 
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return vgAssets }
        
        if let best = filtered.first(where: { url in
            url.contains("src.mp4") && !url.contains("/drvs/") &&
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return best }
        
        if let hls = filtered.first(where: { url in
            url.contains("m3u8") &&
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return hls }
        
        if let vgAny = filtered.first(where: { url in
            url.contains("vg-assets") && url.contains("/videos/") &&
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return vgAny }
        
        if let vgAny2 = filtered.first(where: { url in
            url.contains("vg-assets") &&
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return vgAny2 }
        // –ò—â–µ–º /drvs/*/raw, –Ω–æ –ù–ï md (md = —Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π), –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: hd > sd > raw > –¥—Ä—É–≥–∏–µ
        let drvsVariants = filtered.filter { $0.contains("/drvs/") && $0.contains("/raw") }
        if drvsVariants.isEmpty == false {
            logger?.debug("Found /drvs/*/raw variants: \(drvsVariants.joined(separator: " | "))")
        }
        if let hdRaw = filtered.first(where: { $0.contains("/drvs/hd/raw") }) { return hdRaw }
        if let sdRaw = filtered.first(where: { $0.contains("/drvs/sd/raw") }) { return sdRaw }
        if let rawOnly = filtered.first(where: { $0.contains("/drvs/raw") && !$0.contains("/drvs/md/raw") && !$0.contains("/drvs/hd/raw") && !$0.contains("/drvs/sd/raw") }) { return rawOnly }
        // /drvs/md/raw - —Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –¥—Ä—É–≥–æ–≥–æ –Ω–µ—Ç
        if let mdRaw = filtered.first(where: { $0.contains("/drvs/md/raw") }) { return mdRaw }
        
        // –í–ê–ñ–ù–û: –ü—Ä–æ–≤–µ—Ä—è–µ–º UUID –¥–ª—è –≤—Å–µ—Ö —Å—Å—ã–ª–æ–∫ —Å /az/files/ - –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Å—ã–ª–∫–∏ —Å –¥—Ä—É–≥–∏–º UUID
        if let mp4 = filtered.first(where: { url in
            url.contains(".mp4") &&
            (!url.contains("/az/files/") || mainVideoUUIDs.contains(extractUUIDFromDirectRaw(url) ?? ""))
        }) { return mp4 }
        let toLog = filtered.prefix(3).joined(separator: " | ")
        logger?.debug("Sora candidates (filtered): \(toLog)")
        // –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞—à–ª–∏, –ª–æ–≥–∏—Ä—É–µ–º –≤—Å–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ã –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
        if filtered.isEmpty {
            let allLog = decoded.prefix(5).joined(separator: " | ")
            logger?.debug("Sora all candidates (no video found): \(allLog)")
        }
    }

    return nil
}

// –í—ã—Ç–∞—Å–∫–∏–≤–∞–µ—Ç JSON –∏–∑ Next.js data (—É–ª—É—á—à–µ–Ω–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö)
private func extractNextDataJSON(from html: String) -> String? {
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç <script id="__NEXT_DATA__">...</script>
    // –†–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –∞—Ç—Ä–∏–±—É—Ç–æ–≤ –∏ –∫–∞–≤—ã—á–µ–∫, –º–Ω–æ–≥–æ—Å—Ç—Ä–æ—á–Ω—ã–π JSON
    let pattern1 = #"<script[^>]*id=['\"]__NEXT_DATA__['\"][^>]*>\s*(\{[\s\S]*?\})\s*</script>"#
    if let result = firstCapture(in: html, pattern: pattern1) {
        return result
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: window.__NEXT_DATA__ = {...}
    let pattern2 = #"window\.__NEXT_DATA__\s*=\s*(\{[\s\S]*?\})\s*;"#
    if let result = firstCapture(in: html, pattern: pattern2) {
        return result
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 3: –ü—Ä–æ—Å—Ç–æ __NEXT_DATA__ = {...} (–±–µ–∑ window)
    let pattern3 = #"__NEXT_DATA__\s*=\s*(\{[\s\S]*?\})\s*[;<]"#
    if let result = firstCapture(in: html, pattern: pattern3) {
        return result
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 4: JSON-escaped –∏–ª–∏ URL-encoded –≤–µ—Ä—Å–∏—è
    let decoded = html.removingPercentEncoding ?? html
    if decoded != html {
        // –ü—Ä–æ–±—É–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        if let result = firstCapture(in: decoded, pattern: pattern1) {
            return result
        }
        if let result = firstCapture(in: decoded, pattern: pattern2) {
            return result
        }
        if let result = firstCapture(in: decoded, pattern: pattern3) {
            return result
        }
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5: –ò—â–µ–º –ø—Ä–æ—Å—Ç–æ –±–æ–ª—å—à–æ–π JSON –æ–±—ä–µ–∫—Ç —Å –∫–ª—é—á–æ–º "props" (—Ö–∞—Ä–∞–∫—Ç–µ—Ä–Ω–æ –¥–ª—è Next.js)
    // –≠—Ç–æ –º–µ–Ω–µ–µ –Ω–∞–¥—ë–∂–Ω–æ, –Ω–æ –º–æ–∂–µ—Ç –ø–æ–º–æ—á—å –µ—Å–ª–∏ __NEXT_DATA__ –æ–±—ë—Ä–Ω—É—Ç –ø–æ-–¥—Ä—É–≥–æ–º—É
    let pattern5 = #"<script[^>]*>\s*(\{[^{}]*"props"[\s\S]{100,})\s*</script>"#
    if let result = firstCapture(in: html, pattern: pattern5) {
        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ __NEXT_DATA__ (—Å–æ–¥–µ—Ä–∂–∏—Ç pageProps –∏–ª–∏ buildId)
        if result.contains("pageProps") || result.contains("buildId") || result.contains("__NEXT_DATA__") {
            return result
        }
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5.1: –ò—â–µ–º –≤ URL-encoded –∏–ª–∏ HTML-encoded –≤–∏–¥–µ
    let decodedHtml = html.removingPercentEncoding ?? html
    if decodedHtml != html {
        // –ü—Ä–æ–±—É–µ–º –≤—Å–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã –Ω–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        if let result = firstCapture(in: decodedHtml, pattern: pattern1) {
            return result
        }
        if let result = firstCapture(in: decodedHtml, pattern: pattern2) {
            return result
        }
        if let result = firstCapture(in: decodedHtml, pattern: pattern3) {
            return result
        }
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 5.2: –ò—â–µ–º –≤ HTML entities (&#x5f; = _, &#x4e; = N, –∏ —Ç.–¥.)
    // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º HTML entities –æ–±—Ä–∞—Ç–Ω–æ –≤ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
    let htmlEntitiesPattern = #"&#x([0-9a-fA-F]+);"#
    if let regex = try? NSRegularExpression(pattern: htmlEntitiesPattern, options: []),
       regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)) != nil {
        // –ü—Ä–æ–±—É–µ–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å HTML entities –∏ –∏—Å–∫–∞—Ç—å –≤ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –≤–µ—Ä—Å–∏–∏
        if let decodedMatch = try? NSRegularExpression(pattern: #"&#x5f;&#x5f;NEXT_DATA&#x5f;&#x5f;"#, options: []).firstMatch(in: html, range: NSRange(html.startIndex..., in: html)) {
            // –ù–∞—à–ª–∏ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω—ã–π __NEXT_DATA__, –ø—Ä–æ–±—É–µ–º –Ω–∞–π—Ç–∏ JSON –ø–æ—Å–ª–µ –Ω–µ–≥–æ
            let afterMatch = String(html[html.index(html.startIndex, offsetBy: decodedMatch.range.upperBound)...])
            if let jsonStart = afterMatch.range(of: "{") {
                // –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
                var braceCount = 1
                var endIndex = jsonStart.upperBound
                while braceCount > 0 && endIndex < afterMatch.endIndex {
                    let char = afterMatch[endIndex]
                    if char == "{" {
                        braceCount += 1
                    } else if char == "}" {
                        braceCount -= 1
                    }
                    endIndex = afterMatch.index(endIndex, offsetBy: 1)
                }
                if braceCount == 0 {
                    let jsonStr = String(afterMatch[jsonStart.lowerBound..<endIndex])
                    if jsonStr.contains("pageProps") || jsonStr.contains("downloadable_url") || jsonStr.contains("encodings") {
                        return jsonStr
                    }
                }
            }
        }
    }
    
    // –°—Ç—Ä–∞—Ç–µ–≥–∏—è 6: –ò—â–µ–º –æ—á–µ–Ω—å –±–æ–ª—å—à–æ–π JSON –æ–±—ä–µ–∫—Ç (–º–æ–∂–µ—Ç –±—ã—Ç—å __NEXT_DATA__ –±–µ–∑ —è–≤–Ω–æ–≥–æ —Ç–µ–≥–∞)
    // –ò—â–µ–º –æ–±—ä–µ–∫—Ç—ã —Ä–∞–∑–º–µ—Ä–æ–º –±–æ–ª—å—à–µ 1000 —Å–∏–º–≤–æ–ª–æ–≤, —Å–æ–¥–µ—Ä–∂–∞—â–∏–µ "downloadable_url" –∏–ª–∏ "encodings"
    let largeJsonPattern = #"(\{[^{}]*"(?:downloadable_url|encodings|pageProps)"[\s\S]{1000,})"#
    if let regex = try? NSRegularExpression(pattern: largeJsonPattern, options: [.dotMatchesLineSeparators]),
       let match = regex.firstMatch(in: html, range: NSRange(html.startIndex..., in: html)),
       match.numberOfRanges > 1,
       let range = Range(match.range(at: 1), in: html) {
        var jsonStr = String(html[range])
        // –ü—ã—Ç–∞–µ–º—Å—è –Ω–∞–π—Ç–∏ –∑–∞–∫—Ä—ã–≤–∞—é—â—É—é —Å–∫–æ–±–∫—É
        var braceCount = 1
        var endIndex = range.upperBound
        while braceCount > 0 && endIndex < html.endIndex {
            let char = html[endIndex]
            if char == "{" {
                braceCount += 1
            } else if char == "}" {
                braceCount -= 1
            }
            endIndex = html.index(endIndex, offsetBy: 1)
        }
        if braceCount == 0 {
            jsonStr = String(html[range.lowerBound..<endIndex])
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –ø–æ—Ö–æ–∂–µ –Ω–∞ __NEXT_DATA__
            if jsonStr.contains("pageProps") || jsonStr.contains("downloadable_url") || jsonStr.contains("encodings") {
                return jsonStr
            }
        }
    }
    
    return nil
}

// –ò—â–µ—Ç vg-assets —Å—Å—ã–ª–∫–∏ (–º–æ–≥—É—Ç –±—ã—Ç—å –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö)
// –í–ê–ñ–ù–û: vg-assets —Å—Å—ã–ª–∫–∏ —á–∞—Å—Ç–æ –≤–µ–¥—É—Ç –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—É –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏!
// –í–ê–ñ–ù–û: vg-assets –º–æ–≥—É—Ç –±—ã—Ç—å –∫–∞–∫ /vg-assets/, —Ç–∞–∫ –∏ /az/vg-assets/
private func extractVgAssetsUrl(from html: String, logger: Logger?) -> String? {
    // 1) –ü—Ä—è–º–∞—è —Å—Å—ã–ª–∫–∞ vg-assets —Å src.mp4 (–≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç) - —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/
    let directVgPattern = #"https://videos\.openai\.com(/az)?/vg-assets/[^\s"'<>]+?src\.mp4[^\s"'<>]*"#
    if let found = firstMatch(in: html, pattern: directVgPattern) {
        logger?.info("‚úÖ Found vg-assets with src.mp4: \(found.prefix(200))...")
        return decodePotentiallyEncodedURL(found)
    }
    
    // 2) Percent-encoded vg-assets - —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/
    let percentVgPattern = #"https%3A%2F%2Fvideos\.openai\.com(%2Faz)?%2Fvg-assets%2F[^&"'<>\s]+src\.mp4[^&"'<>\s]*"#
    if let found = firstMatch(in: html, pattern: percentVgPattern) {
        let decoded = decodePotentiallyEncodedURL(found)
        if decoded.contains("src.mp4") {
            logger?.info("‚úÖ Found percent-encoded vg-assets with src.mp4: \(decoded.prefix(200))...")
            return decoded
        }
    }
    
    // 3) JSON-escaped vg-assets - —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/
    let jsonVgPattern = #"https:\/\/videos\.openai\.com(\/az)?\/vg-assets\/[^"]+src\.mp4[^"]*"#
    if let found = firstMatch(in: html, pattern: jsonVgPattern) {
        let decoded = decodePotentiallyEncodedURL(found)
        if decoded.contains("src.mp4") {
            logger?.info("‚úÖ Found JSON-escaped vg-assets with src.mp4: \(decoded.prefix(200))...")
            return decoded
        }
    }
    
    // 4) –°–æ–±–∏—Ä–∞–µ–º –í–°–ï vg-assets —Å—Å—ã–ª–∫–∏ –∏ –≤—ã–±–∏—Ä–∞–µ–º –ª—É—á—à—É—é (–Ω–µ —Ç–æ–ª—å–∫–æ —Å src.mp4)
    // –í–ê–ñ–ù–û: —É—á–∏—Ç—ã–≤–∞–µ–º –∏ /az/vg-assets/ —Ç–æ–∂–µ!
    let anyVgPattern = #"https://videos\.openai\.com(/az)?/vg-assets/[^\s"'<>\\]+"#
    let candidates = allMatches(in: html, pattern: anyVgPattern)
    if candidates.isEmpty == false {
        let decoded = candidates.map { decodePotentiallyEncodedURL($0) }
        logger?.info("üîç Found \(decoded.count) vg-assets candidates: \(decoded.prefix(5).map { $0.prefix(150) }.joined(separator: " | "))")
        // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: src.mp4 > .mp4 > m3u8 > –ª—é–±—ã–µ –¥—Ä—É–≥–∏–µ vg-assets (–Ω–æ –Ω–µ thumbnail/jpeg)
        if let best = decoded.first(where: { 
            $0.contains("src.mp4") && 
            !$0.contains("thumbnail") && 
            !$0.contains(".jpeg") && 
            !$0.contains(".jpg")
        }) { 
            logger?.info("‚úÖ Found vg-assets with src.mp4 (from all candidates): \(best.prefix(200))...")
            return best 
        }
        if let mp4 = decoded.first(where: { 
            $0.contains(".mp4") && 
            !$0.contains("thumbnail") && 
            !$0.contains(".jpeg") && 
            !$0.contains(".jpg")
        }) { 
            logger?.info("‚úÖ Found vg-assets with .mp4: \(mp4.prefix(200))...")
            return mp4 
        }
        if let m3u8 = decoded.first(where: { 
            $0.contains("m3u8") && 
            !$0.contains("thumbnail")
        }) { 
            logger?.info("‚úÖ Found vg-assets with m3u8: \(m3u8.prefix(200))...")
            return m3u8 
        }
        // –ü–æ—Å–ª–µ–¥–Ω–∏–π –≤–∞—Ä–∏–∞–Ω—Ç: –ª—é–±—ã–µ vg-assets (–Ω–æ –Ω–µ thumbnail/jpeg)
        if let anyVg = decoded.first(where: { 
            !$0.contains("thumbnail") && 
            !$0.contains(".jpeg") && 
            !$0.contains(".jpg") &&
            !$0.contains(".png")
        }) {
            logger?.info("‚úÖ Found vg-assets (any video): \(anyVg.prefix(200))...")
            return anyVg
        }
        
        // –ü–û–°–õ–ï–î–ù–Ø–Ø –ü–û–ü–´–¢–ö–ê: –µ—Å–ª–∏ –≤—Å–µ vg-assets —Å—Å—ã–ª–∫–∏ - thumbnail, –ø–æ–ø—Ä–æ–±—É–µ–º –∑–∞–º–µ–Ω–∏—Ç—å #thumbnail –Ω–∞ #src.mp4
        // –í–ê–ñ–ù–û: –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –í–°–ï –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞ –ø–æ—Å–ª–µ ?
        if let thumbnailVg = decoded.first(where: { 
            $0.contains("vg-assets") && 
            ($0.contains("#thumbnail") || $0.contains("thumbnail"))
        }) {
            // –†–∞–∑–¥–µ–ª—è–µ–º URL –Ω–∞ –ø—É—Ç—å –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∑–∞–ø—Ä–æ—Å–∞
            let urlParts = thumbnailVg.split(separator: "?", maxSplits: 1)
            var pathPart = String(urlParts[0])
            let queryPart = urlParts.count > 1 ? "?" + String(urlParts[1]) : ""
            
            // –í–∞—Ä–∏–∞–Ω—Ç 1: –∑–∞–º–µ–Ω–∏—Ç—å #thumbnail.jpeg –Ω–∞ #src.mp4
            if pathPart.contains("#thumbnail.jpeg") {
                pathPart = pathPart.replacingOccurrences(of: "#thumbnail.jpeg", with: "#src.mp4")
                let modifiedUrl = pathPart + queryPart
                logger?.info("üîß Attempting to modify vg-assets URL: replaced #thumbnail.jpeg with #src.mp4 (preserving all query params)")
                logger?.info("üîß Modified URL: \(modifiedUrl.prefix(200))...")
                return modifiedUrl
            }
            
            // –í–∞—Ä–∏–∞–Ω—Ç 2: –∑–∞–º–µ–Ω–∏—Ç—å #thumbnail –Ω–∞ #src.mp4
            if pathPart.contains("#thumbnail") {
                pathPart = pathPart.replacingOccurrences(of: "#thumbnail", with: "#src.mp4")
                let modifiedUrl = pathPart + queryPart
                logger?.info("üîß Attempting to modify vg-assets URL: replaced #thumbnail with #src.mp4 (preserving all query params)")
                logger?.info("üîß Modified URL: \(modifiedUrl.prefix(200))...")
                return modifiedUrl
            }
            
            // –í–∞—Ä–∏–∞–Ω—Ç 3: –∑–∞–º–µ–Ω–∏—Ç—å thumbnail.jpeg –Ω–∞ src.mp4 –≤ –ª—é–±–æ–º –º–µ—Å—Ç–µ
            if pathPart.contains("thumbnail.jpeg") {
                pathPart = pathPart.replacingOccurrences(of: "thumbnail.jpeg", with: "src.mp4")
                let modifiedUrl = pathPart + queryPart
                logger?.info("üîß Attempting to modify vg-assets URL: replaced thumbnail.jpeg with src.mp4 (preserving all query params)")
                logger?.info("üîß Modified URL: \(modifiedUrl.prefix(200))...")
                return modifiedUrl
            }
            
            // –í–∞—Ä–∏–∞–Ω—Ç 4: —É–¥–∞–ª–∏—Ç—å #thumbnail —á–∞—Å—Ç—å, –æ—Å—Ç–∞–≤–∏–≤ #file_ –∏ –¥–æ–±–∞–≤–∏–≤ #src.mp4
            if let fileIndex = pathPart.range(of: "#file_") {
                let beforeFile = String(pathPart[..<fileIndex.upperBound])
                // –î–æ–±–∞–≤–ª—è–µ–º src.mp4 –ø–æ—Å–ª–µ #file_...
                if let hashAfterFile = pathPart[fileIndex.upperBound...].firstIndex(of: "#") {
                    // –ï—Å—Ç—å –µ—â–µ –æ–¥–∏–Ω # –ø–æ—Å–ª–µ #file_, –∑–∞–º–µ–Ω—è–µ–º —á–∞—Å—Ç—å –ø–æ—Å–ª–µ –Ω–µ–≥–æ
                    let afterFileHash = String(pathPart[..<hashAfterFile])
                    let modifiedUrl = afterFileHash + "#src.mp4" + queryPart
                    logger?.info("üîß Attempting to modify vg-assets URL: removed thumbnail part, added #src.mp4 (preserving all query params)")
                    logger?.info("üîß Modified URL: \(modifiedUrl.prefix(200))...")
                    return modifiedUrl
                } else {
                    // –ù–µ—Ç # –ø–æ—Å–ª–µ #file_, –ø—Ä–æ—Å—Ç–æ –¥–æ–±–∞–≤–ª—è–µ–º #src.mp4
                    let modifiedUrl = beforeFile + "file_" + String(pathPart[fileIndex.upperBound...]) + "#src.mp4" + queryPart
                    logger?.info("üîß Attempting to modify vg-assets URL: added #src.mp4 (preserving all query params)")
                    logger?.info("üîß Modified URL: \(modifiedUrl.prefix(200))...")
                    return modifiedUrl
                }
            }
        }
        
        logger?.debug("vg-assets candidates (all): \(decoded.prefix(5).joined(separator: ", "))")
    }
    
    return nil
}

// –ü–∞—Ä—Å–∏–º JSON –∏ –∏—â–µ–º –ª—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ videos.openai.com; –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π mp4/hls
private func extractFromNextData(_ jsonString: String, logger: Logger?) -> String? {
    guard let data = jsonString.data(using: .utf8) else { return nil }
    guard let obj = try? JSONSerialization.jsonObject(with: data) else { return nil }
    var collected: [String] = []
    var priorityUrls: [String] = [] // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ —Å—Å—ã–ª–∫–∏ (vg-assets, src.mp4)
    var downloadableUrl: String? = nil // downloadable_url - —Å–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
    var encodingSourcePath: String? = nil // encodings.source.path - –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç

    func walk(_ value: Any, path: String = "") {
        if let dict = value as? [String: Any] {
            // –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–ª—é—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Å—Å—ã–ª–∫–∏ –Ω–∞ –≤–∏–¥–µ–æ
            for (key, v) in dict {
                let currentPath = path.isEmpty ? key : "\(path).\(key)"
                // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è –≤–∏–¥–µ–æ
                let keyLower = key.lowercased()
                
                // –°–ø–µ—Ü–∏–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è encodings - —Ç–∞–º –º–æ–∂–µ—Ç –±—ã—Ç—å source (–±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏) –∏ source_wm (—Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π)
                if keyLower == "encodings" {
                    if let encodingsDict = v as? [String: Any] {
                        logger?.debug("Found encodings dict at path '\(currentPath)', keys: \(encodingsDict.keys.joined(separator: ", "))")
                        
                        // –ò—â–µ–º source.path (–æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏) - –≤—ã—Å—à–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç
                        if let source = encodingsDict["source"] as? [String: Any] {
                            logger?.debug("Found encodings.source at path '\(currentPath)', keys: \(source.keys.joined(separator: ", "))")
                            if let path = source["path"] as? String,
                               path.contains("videos.openai.com") && !path.contains("sora.chatgpt.com") {
                                let decoded = decodePotentiallyEncodedURL(path)
                                if !decoded.contains("thumbnail") && !decoded.contains(".jpeg") && !decoded.contains(".jpg") && !decoded.contains(".png") {
                                    encodingSourcePath = decoded // –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Å–∞–º–æ–≥–æ –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                                    priorityUrls.insert(decoded, at: 0) // –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ priorityUrls
                                    logger?.info("‚úÖ Found encodings.source.path (original without watermark) at path '\(currentPath)': \(decoded)")
                                }
                            }
                        }
                        // source_wm - —Å –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–æ–π, –Ω–æ —Ç–æ–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–æ–ª–µ–∑–µ–Ω
                        if let sourceWm = encodingsDict["source_wm"] as? [String: Any] {
                            if let path = sourceWm["path"] as? String,
                               path.contains("videos.openai.com") && !path.contains("sora.chatgpt.com") {
                                let decoded = decodePotentiallyEncodedURL(path)
                                if !decoded.contains("thumbnail") && !decoded.contains(".jpeg") && !decoded.contains(".jpg") && !decoded.contains(".png") {
                                    collected.append(decoded)
                                    logger?.debug("Found encodings.source_wm.path (with watermark) at path '\(currentPath)': \(decoded)")
                                }
                            }
                        }
                    }
                }
                
                if keyLower == "downloadable_url" || keyLower == "downloadableurl" {
                    // downloadable_url —á–∞—Å—Ç–æ –≤–µ–¥—ë—Ç –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—É –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏
                    if let s = v as? String, s.contains("videos.openai.com") && !s.contains("sora.chatgpt.com") {
                        let decoded = decodePotentiallyEncodedURL(s)
                        // –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ —ç—Ç–æ –Ω–µ thumbnail
                        if !decoded.contains("thumbnail") && !decoded.contains(".jpeg") && !decoded.contains(".jpg") && !decoded.contains(".png") {
                            downloadableUrl = decoded // –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ç–¥–µ–ª—å–Ω–æ –¥–ª—è —Å–∞–º–æ–≥–æ –≤—ã—Å–æ–∫–æ–≥–æ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞
                            priorityUrls.insert(decoded, at: 0) // –¢–∞–∫–∂–µ –¥–æ–±–∞–≤–ª—è–µ–º –≤ priorityUrls
                            logger?.info("‚úÖ Found downloadable_url at path '\(currentPath)': \(decoded)")
                        }
                    }
                } else if ["videoUrl", "video_url", "source", "url", "src", "video", "mp4", "mediaUrl", "media_url"].contains(keyLower) {
                    if let s = v as? String, s.contains("videos.openai.com") {
                        let decoded = decodePotentiallyEncodedURL(s)
                        if decoded.contains("vg-assets") || decoded.contains("src.mp4") {
                            priorityUrls.append(decoded)
                        } else {
                            collected.append(decoded)
                        }
                    }
                }
                walk(v, path: currentPath)
            }
        } else if let arr = value as? [Any] {
            for (idx, v) in arr.enumerated() {
                walk(v, path: "\(path)[\(idx)]")
            }
        } else if let s = value as? String {
            // –ò—Å–∫–ª—é—á–∞–µ–º —Å—Å—ã–ª–∫–∏ –Ω–∞ sora.chatgpt.com - –Ω–∞–º –Ω—É–∂–Ω—ã —Ç–æ–ª—å–∫–æ videos.openai.com
            if s.contains("videos.openai.com") && !s.contains("sora.chatgpt.com") {
                let decoded = decodePotentiallyEncodedURL(s)
                // –§–∏–ª—å—Ç—Ä—É–µ–º thumbnail –∏ –¥—Ä—É–≥–∏–µ –Ω–µ-–≤–∏–¥–µ–æ
                if !decoded.contains("thumbnail") && !decoded.contains(".jpeg") && !decoded.contains(".jpg") && !decoded.contains(".png") {
                    if decoded.contains("vg-assets") || decoded.contains("src.mp4") {
                        priorityUrls.append(decoded)
                    } else {
                        collected.append(decoded)
                    }
                }
            }
        }
    }
    walk(obj)
    
    logger?.info("extractFromNextData: found \(priorityUrls.count) priority URLs, \(collected.count) collected URLs")
    if priorityUrls.isEmpty == false {
        logger?.info("Priority URLs: \(priorityUrls.prefix(5).joined(separator: " | "))")
    }
    
    // –°–ê–ú–´–ô –í–´–°–û–ö–ò–ô –ü–†–ò–û–†–ò–¢–ï–¢: downloadable_url –∏ encodings.source.path (–æ–Ω–∏ —á–∞—Å—Ç–æ –≤–µ–¥—É—Ç –∫ –æ—Ä–∏–≥–∏–Ω–∞–ª—É –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏)
    if let downloadable = downloadableUrl {
        logger?.info("üéØ Using downloadable_url (highest priority): \(downloadable)")
        return downloadable
    }
    if let encoding = encodingSourcePath {
        logger?.info("üéØ Using encodings.source.path (highest priority): \(encoding)")
        return encoding
    }

    // –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ
    if priorityUrls.isEmpty == false {
        // –°–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - /az/files/{id}/raw –ë–ï–ó /drvs/ (–æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏)
        // –°–æ—Ä—Ç–∏—Ä—É–µ–º: —Å–Ω–∞—á–∞–ª–∞ /az/files/{uuid}/raw –±–µ–∑ /drvs/, –ø–æ—Ç–æ–º –æ—Å—Ç–∞–ª—å–Ω—ã–µ
        let sorted = priorityUrls.sorted { url1, url2 in
            let isDirectRaw1 = url1.contains("/az/files/") && url1.contains("/raw") && !url1.contains("/drvs/")
            let isDirectRaw2 = url2.contains("/az/files/") && url2.contains("/raw") && !url2.contains("/drvs/")
            if isDirectRaw1 && !isDirectRaw2 { return true }
            if !isDirectRaw1 && isDirectRaw2 { return false }
            return url1 < url2
        }
        
        // –î–ª—è /az/files/{uuid}/raw –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö SAS-–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        if let directRaw = sorted.first(where: { 
            $0.contains("/az/files/") && 
            $0.contains("/raw") && 
            !$0.contains("/drvs/") &&
            ($0.contains("?se=") || $0.contains("&se=")) && 
            ($0.contains("?sp=") || $0.contains("&sp=")) && 
            ($0.contains("?sv=") || $0.contains("&sv=")) && 
            ($0.contains("?sr=") || $0.contains("&sr=")) && 
            ($0.contains("?sig=") || $0.contains("&sig=")) && 
            ($0.contains("?ac=") || $0.contains("&ac="))
        }) {
            logger?.info("Found validated direct /az/files/{id}/raw from priorityUrls (original without watermark): \(directRaw)")
            return directRaw
        }
        
        // –î–ª—è –¥—Ä—É–≥–∏—Ö —Ç–∏–ø–æ–≤ —Å—Å—ã–ª–æ–∫ –≤–∞–ª–∏–¥–∞—Ü–∏—è –Ω–µ –Ω—É–∂–Ω–∞
        if let best = sorted.first(where: { $0.contains("vg-assets") && $0.contains("src.mp4") }) { return best }
        if let vg = sorted.first(where: { $0.contains("vg-assets") && $0.contains("/videos/") }) { return vg }
        if let vgAny = sorted.first(where: { $0.contains("vg-assets") }) { return vgAny }
        if let src = sorted.first(where: { $0.contains("src.mp4") }) { return src }
        // –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ –ø–æ–¥–æ—à–ª–æ, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø–µ—Ä–≤—É—é –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö (–Ω–æ –Ω–µ /az/files/{uuid}/raw –±–µ–∑ –≤–∞–ª–∏–¥–∞—Ü–∏–∏)
        if let first = sorted.first, !(first.contains("/az/files/") && first.contains("/raw") && !first.contains("/drvs/")) {
            logger?.info("Using first priority URL: \(first)")
            return first
        }
    }

    // –ó–∞—Ç–µ–º –æ–±—ã—á–Ω—ã–µ
    if collected.isEmpty == false {
        // –°–∞–º—ã–π –≤—ã—Å–æ–∫–∏–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç - /az/files/{id}/raw –ë–ï–ó /drvs/ (–æ—Ä–∏–≥–∏–Ω–∞–ª –±–µ–∑ –≤–∞—Ç–µ—Ä–º–∞—Ä–∫–∏)
        // –ù–æ —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –µ—Å—Ç—å –≤—Å–µ SAS-–ø–∞—Ä–∞–º–µ—Ç—Ä—ã
        if let directRaw = collected.first(where: { 
            $0.contains("/az/files/") && 
            $0.contains("/raw") && 
            !$0.contains("/drvs/") &&
            ($0.contains("?se=") || $0.contains("&se=")) && 
            ($0.contains("?sp=") || $0.contains("&sp=")) && 
            ($0.contains("?sv=") || $0.contains("&sv=")) && 
            ($0.contains("?sr=") || $0.contains("&sr=")) && 
            ($0.contains("?sig=") || $0.contains("&sig=")) && 
            ($0.contains("?ac=") || $0.contains("&ac="))
        }) {
            logger?.info("Found validated direct /az/files/{id}/raw from collected (original without watermark)")
            return directRaw
        }
        if let best = collected.first(where: { $0.contains("src.mp4") }) { return best }
        if let vg = collected.first(where: { $0.contains("vg-assets") && $0.contains("/videos/") }) { return vg }
        if let vgAny = collected.first(where: { $0.contains("vg-assets") }) { return vgAny }
        if let mp4 = collected.first(where: { $0.contains(".mp4") }) { return mp4 }
        if let hls = collected.first(where: { $0.contains("m3u8") }) { return hls }
        logger?.debug("NEXT_DATA candidates: \(collected.prefix(3).joined(separator: " | "))")
    }
    return nil
}

private func firstMatch(in text: String, pattern: String) -> String? {
    guard let regex = try? NSRegularExpression(pattern: pattern, options: []) else { return nil }
    let range = NSRange(text.startIndex..<text.endIndex, in: text)
    guard let match = regex.firstMatch(in: text, options: [], range: range), let r = Range(match.range, in: text) else {
        return nil
    }
    return String(text[r])
}

private func firstCapture(in text: String, pattern: String) -> String? {
    guard let regex = try? NSRegularExpression(pattern: pattern, options: [.caseInsensitive, .dotMatchesLineSeparators]) else { return nil }
    let range = NSRange(text.startIndex..<text.endIndex, in: text)
    guard let match = regex.firstMatch(in: text, options: [], range: range), match.numberOfRanges > 1,
          let r = Range(match.range(at: 1), in: text) else { return nil }
    return String(text[r])
}

private func allMatches(in text: String, pattern: String) -> [String] {
    guard let regex = try? NSRegularExpression(pattern: pattern, options: []) else { return [] }
    let range = NSRange(text.startIndex..<text.endIndex, in: text)
    let matches = regex.matches(in: text, options: [], range: range)
    return matches.compactMap { m in
        guard let r = Range(m.range, in: text) else { return nil }
        return String(text[r])
    }
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç UUID –∏–∑ JSON –¥–∞–Ω–Ω—ã—Ö (–∏—â–µ—Ç –≤ –∫–ª—é—á–∞—Ö —Ç–∏–ø–∞ fileId, id, uuid, videoId –∏ —Ç.–¥.)
private func extractUUIDFromJSON(_ jsonString: String, logger: Logger?) -> String? {
    guard let data = jsonString.data(using: .utf8) else { return nil }
    guard let obj = try? JSONSerialization.jsonObject(with: data) else { return nil }
    
    var candidateUUIDs: [String] = []
    var priorityUUIDs: [String] = [] // UUID –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö –∫–ª—é—á–µ–π
    
    func walk(_ value: Any, path: String = "") {
        if let dict = value as? [String: Any] {
            for (key, v) in dict {
                let currentPath = path.isEmpty ? key : "\(path).\(key)"
                // –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∫–ª—é—á–∏ –¥–ª—è UUID —Ñ–∞–π–ª–∞
                if ["fileId", "file_id", "id", "uuid", "videoId", "video_id", "assetId", "asset_id", "fileUuid", "file_uuid", "mediaId", "media_id", "contentId", "content_id"].contains(key.lowercased()) {
                    if let s = v as? String, isValidUUID(s) {
                        priorityUUIDs.append(s)
                        logger?.debug("Found UUID candidate in key '\(key)' at path '\(currentPath)': \(s)")
                    }
                }
                walk(v, path: currentPath)
            }
        } else if let arr = value as? [Any] {
            for (idx, v) in arr.enumerated() {
                walk(v, path: "\(path)[\(idx)]")
            }
        } else if let s = value as? String {
            // –ò—â–µ–º UUID –≤ —Å—Ç—Ä–æ–∫–∞—Ö (–º–æ–≥—É—Ç –±—ã—Ç—å –≤ URL –∏–ª–∏ –∫–∞–∫ –∑–Ω–∞—á–µ–Ω–∏—è)
            if isValidUUID(s) {
                candidateUUIDs.append(s)
            }
        }
    }
    walk(obj)
    
    // –°–Ω–∞—á–∞–ª–∞ –ø—Ä–æ–≤–µ—Ä—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ UUID
    if priorityUUIDs.isEmpty == false {
        // –í—ã–±–∏—Ä–∞–µ–º UUID, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "00000000" (—ç—Ç–æ —á–∞—Å—Ç–æ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è —Ñ–∞–π–ª–æ–≤)
        if let prefixed = priorityUUIDs.first(where: { $0.hasPrefix("00000000-") }) {
            logger?.info("Selected priority UUID with 00000000- prefix: \(prefixed)")
            return prefixed
        }
        // –ò–Ω–∞—á–µ –ø–µ—Ä–≤—ã–π –∏–∑ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã—Ö
        logger?.info("Selected first priority UUID: \(priorityUUIDs.first!)")
        return priorityUUIDs.first
    }
    
    // –ó–∞—Ç–µ–º –ø—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ –æ—Å—Ç–∞–ª—å–Ω—ã–µ
    if candidateUUIDs.isEmpty == false {
        // –í—ã–±–∏—Ä–∞–µ–º UUID, –∫–æ—Ç–æ—Ä—ã–π –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å "00000000"
        if let prefixed = candidateUUIDs.first(where: { $0.hasPrefix("00000000-") }) {
            logger?.info("Selected UUID with 00000000- prefix from all candidates: \(prefixed)")
            return prefixed
        }
        logger?.debug("Selected first UUID from all candidates: \(candidateUUIDs.first!)")
        return candidateUUIDs.first
    }
    
    return nil
}

// –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —Å—Ç—Ä–æ–∫–∞ –≤–∞–ª–∏–¥–Ω—ã–º UUID
private func isValidUUID(_ s: String) -> Bool {
    let pattern = #"^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$"#
    return (try? NSRegularExpression(pattern: pattern, options: .caseInsensitive).firstMatch(in: s, options: [], range: NSRange(s.startIndex..<s.endIndex, in: s))) != nil
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤—Å–µ UUID –∏–∑ —Ç–µ–∫—Å—Ç–∞ (–¥–ª—è –æ—Ç–ª–∞–¥–∫–∏)
// –ò—â–µ—Ç UUID –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö: –æ–±—ã—á–Ω—ã–π, percent-encoded, JSON-escaped
private func extractAllUUIDs(from text: String) -> [String] {
    var foundUUIDs: Set<String> = []
    
    // 1. –û–±—ã—á–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç UUID
    let pattern = #"[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}"#
    if let regex = try? NSRegularExpression(pattern: pattern, options: .caseInsensitive) {
        let range = NSRange(text.startIndex..<text.endIndex, in: text)
        let matches = regex.matches(in: text, options: [], range: range)
        for match in matches {
            if let r = Range(match.range, in: text) {
                foundUUIDs.insert(String(text[r]))
            }
        }
    }
    
    // 2. Percent-encoded —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, %30%30%30%30%30%30%30%30-%33%63%38%63)
    // –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω –≤–∏–¥–∞: %30%30%30%30%30%30%30%30-%33%63%38%63-...
    let percentPattern = #"(?:%[0-9a-fA-F]{2}){8}-(?:%[0-9a-fA-F]{2}){4}-(?:%[0-9a-fA-F]{2}){4}-(?:%[0-9a-fA-F]{2}){4}-(?:%[0-9a-fA-F]{2}){12}"#
    if let regex = try? NSRegularExpression(pattern: percentPattern, options: []) {
        let range = NSRange(text.startIndex..<text.endIndex, in: text)
        let matches = regex.matches(in: text, options: [], range: range)
        for match in matches {
            if let r = Range(match.range, in: text) {
                let encoded = String(text[r])
                if let decoded = encoded.removingPercentEncoding, isValidUUID(decoded) {
                    foundUUIDs.insert(decoded)
                }
            }
        }
    }
    
    // 3. JSON-escaped —Ñ–æ—Ä–º–∞—Ç (–Ω–∞–ø—Ä–∏–º–µ—Ä, \u0030\u0030\u0030\u0030\u0030\u0030\u0030\u0030-\u0033\u0063\u0038\u0063)
    // –≠—Ç–æ —Å–ª–æ–∂–Ω–µ–µ, –Ω–æ –º–æ–∂–µ–º –ø–æ–ø—Ä–æ–±–æ–≤–∞—Ç—å –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞—Ç—å –≤—Å—é —Å—Ç—Ä–æ–∫—É –∏ –∏—Å–∫–∞—Ç—å UUID
    
    return Array(foundUUIDs)
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç –í–°–ï —Å—Å—ã–ª–∫–∏ videos.openai.com –∏–∑ HTML —Å —Ä–∞–∑–Ω—ã–º–∏ –≤–∞—Ä–∏–∞–Ω—Ç–∞–º–∏ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è
private func extractAllVideoUrls(from html: String) -> [String] {
    var allUrls: Set<String> = []
    
    // 1. –ü—Ä—è–º—ã–µ —Å—Å—ã–ª–∫–∏: https://videos.openai.com/...
    // –£–ª—É—á—à–µ–Ω–Ω—ã–π –ø–∞—Ç—Ç–µ—Ä–Ω: –Ω–µ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–π —Å–∫–æ–±–∫–µ, –µ—Å–ª–∏ –æ–Ω–∞ —á–∞—Å—Ç—å URL-–ø–∞—Ä–∞–º–µ—Ç—Ä–∞
    // –ò—â–µ–º –¥–æ –ø—Ä–æ–±–µ–ª–∞, –¥–≤–æ–π–Ω–æ–π –∫–∞–≤—ã—á–∫–∏, –æ–¥–∏–Ω–∞—Ä–Ω–æ–π –∫–∞–≤—ã—á–∫–∏, < –∏–ª–∏ >, –Ω–æ –Ω–µ –Ω–∞ )
    let directPattern = #"https://videos\.openai\.com[^\s"'<>]+"#
    if let regex = try? NSRegularExpression(pattern: directPattern, options: []) {
        let range = NSRange(html.startIndex..<html.endIndex, in: html)
        let matches = regex.matches(in: html, options: [], range: range)
        for match in matches {
            if let r = Range(match.range, in: html) {
                let url = String(html[r])
                let decoded = decodePotentiallyEncodedURL(url)
                if decoded.contains("videos.openai.com") {
                    allUrls.insert(decoded)
                }
            }
        }
    }
    
    // 2. JSON-escaped: https:\/\/videos.openai.com...
    // –í–ê–ñ–ù–û: –ù–ï –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º—Å—è –Ω–∞ –ø—Ä–æ–±–µ–ª–∞—Ö –≤–Ω—É—Ç—Ä–∏ URL - –¥–æ–±–∞–≤–ª—è–µ–º \s –≤ –∏—Å–∫–ª—é—á–µ–Ω–∏—è
    let jsonEscapedPattern = #"https:\\?/\\?/videos\.openai\.com[^"\\\s]+"#
    if let regex = try? NSRegularExpression(pattern: jsonEscapedPattern, options: []) {
        let range = NSRange(html.startIndex..<html.endIndex, in: html)
        let matches = regex.matches(in: html, options: [], range: range)
        for match in matches {
            if let r = Range(match.range, in: html) {
                let url = String(html[r])
                let decoded = decodePotentiallyEncodedURL(url)
                if decoded.contains("videos.openai.com") {
                    allUrls.insert(decoded)
                }
            }
        }
    }
    
    // 3. Percent-encoded: https%3A%2F%2Fvideos.openai.com...
    let percentPattern = #"https%3A%2F%2Fvideos\.openai\.com[^&"'<>\s]+"#
    if let regex = try? NSRegularExpression(pattern: percentPattern, options: []) {
        let range = NSRange(html.startIndex..<html.endIndex, in: html)
        let matches = regex.matches(in: html, options: [], range: range)
        for match in matches {
            if let r = Range(match.range, in: html) {
                let url = String(html[r])
                let decoded = decodePotentiallyEncodedURL(url)
                if decoded.contains("videos.openai.com") {
                    allUrls.insert(decoded)
                }
            }
        }
    }
    
    return Array(allUrls)
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç UUID –∏–∑ URL –≤–∏–¥–∞ /az/files/{hash}_{uuid}/drvs/...
private func extractUUIDFromDrvsUrl(_ url: String) -> String? {
    // –ü–∞—Ç—Ç–µ—Ä–Ω: /az/files/{hash}_{uuid}/drvs/
    let pattern = #"/az/files/[^/]+_([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})/drvs/"#
    return firstCapture(in: url, pattern: pattern)
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç UUID –∏–∑ –ø—Ä—è–º–æ–π —Å—Å—ã–ª–∫–∏ –≤–∏–¥–∞ /az/files/{uuid}/raw
private func extractUUIDFromDirectRaw(_ url: String) -> String? {
    // –ü–∞—Ç—Ç–µ—Ä–Ω: /az/files/{uuid}/raw (–º–æ–∂–µ—Ç –±—ã—Ç—å percent-encoded –∫–∞–∫ %2Fraw)
    let pattern1 = #"/az/files/([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})/raw"#
    if let uuid = firstCapture(in: url, pattern: pattern1) {
        return uuid
    }
    // Percent-encoded –≤–∞—Ä–∏–∞–Ω—Ç
    let pattern2 = #"/az/files/([0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12})%2Fraw"#
    return firstCapture(in: url, pattern: pattern2)
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç query –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–∑ URL
private func extractParamsFromUrl(_ url: String) -> String? {
    guard let queryRange = url.range(of: "?") else { return nil }
    let queryString = String(url[queryRange.upperBound...])
    return queryString.isEmpty ? nil : queryString
}

// –ò–∑–≤–ª–µ–∫–∞–µ—Ç task_id –∏–∑ HTML/JSON (—Ñ–æ—Ä–º–∞—Ç: task_01k7aaa5ryfngt37ys2fe11jg7)
private func extractTaskId(from text: String) -> String? {
    let pattern = #"task_[a-z0-9]+"#
    guard let regex = try? NSRegularExpression(pattern: pattern, options: .caseInsensitive) else { return nil }
    let range = NSRange(text.startIndex..<text.endIndex, in: text)
    guard let match = regex.firstMatch(in: text, options: [], range: range),
          let r = Range(match.range, in: text) else { return nil }
    return String(text[r])
}

// –î–µ–∫–æ–¥–∏—Ä—É–µ—Ç —Å–æ—á–µ—Ç–∞–Ω–∏—è JSON-—ç—Å–∫–µ–π–ø–æ–≤, HTML-—ç–Ω–∫–æ–¥–∏–Ω–≥–∞ –∏ percent-encoding
private func decodePotentiallyEncodedURL(_ s: String) -> String {
    var result = s
    // –£–¥–∞–ª—è–µ–º –≤–æ–∑–º–æ–∂–Ω—ã–µ –∑–∞–≤–µ—Ä—à–∞—é—â–∏–µ –∫–∞–≤—ã—á–∫–∏/–ø—Ä–æ–±–µ–ª—ã/–æ–±—Ä–∞—Ç–Ω—ã–µ —Å–ª—ç—à–∏
    result = result.trimmingCharacters(in: CharacterSet.whitespacesAndNewlines.union(CharacterSet(charactersIn: "'\"\\")))
    // JSON escapes - –¥–µ–∫–æ–¥–∏—Ä—É–µ–º –≤—Å–µ —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—ë–Ω–Ω—ã–µ Unicode escape-–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    result = result.replacingOccurrences(of: #"\u0026"#, with: "&")  // & - –í–ê–ñ–ù–û –¥–ª—è JSON-escaped –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ URL
    result = result.replacingOccurrences(of: #"\u002F"#, with: "/")  // /
    result = result.replacingOccurrences(of: #"\u003D"#, with: "=")  // =
    result = result.replacingOccurrences(of: #"\u003F"#, with: "?")   // ?
    result = result.replacingOccurrences(of: #"\/"#, with: "/")
    // HTML entities
    result = result.replacingOccurrences(of: "&amp;", with: "&")
    result = result.replacingOccurrences(of: "&quot;", with: "\"")
    // Percent-decoding (–¥–≤–∞–∂–¥—ã –Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π)
    if let once = result.removingPercentEncoding { result = once }
    if let twice = result.removingPercentEncoding { result = twice }
    // –£–¥–∞–ª—è–µ–º –æ–±—Ä–∞—Ç–Ω—ã–π —Å–ª—ç—à –≤ –∫–æ–Ω—Ü–µ (–µ—Å–ª–∏ –æ—Å—Ç–∞–ª—Å—è –ø–æ—Å–ª–µ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è)
    if result.hasSuffix("\\") {
        result = String(result.dropLast())
    }
    return result
}

private func sendTelegramMessage(token: String, chatId: Int64, text: String, client: Client) async throws -> Bool {
    struct Payload: Content {
        let chat_id: Int64
        let text: String
        let disable_web_page_preview: Bool
    }
    let payload = Payload(chat_id: chatId, text: text, disable_web_page_preview: false)
    let url = "https://api.telegram.org/bot\(token)/sendMessage"
    let res = try await client.post(URI(string: url)) { req in
        try req.content.encode(payload, as: .json)
    }
    return res.status == .ok
}

